

## 1. Systèmes agentiques : architectures et principes cognitifs

Les **systèmes agentiques** combinent des _LLM_ avec des boucles de décision et des actions dans un environnement. L’architecture **ReAct** (Reason+Act) en est un paradigme clé : elle **intercale des étapes de raisonnement et des actions** (par ex. recherches, calculs) pour résoudre un objectif. Concrètement, un agent ReAct génère une pensée intermédiaire (“raisonnement”) puis une commande d’action, reçoit une observation en retour, et poursuit ainsi en plusieurs itérations. Ce mélange permet de **mieux planifier et interagir** avec le monde : ReAct surpasse les approches purement réflexives ou purement actives et produit des trajectoires de résolution alignées sur celles des humains (facilitant l’**interprétabilité** et le diagnostic). Des frameworks tels que **LangChain** implémentent ces agents en chaîne, offrant une infrastructure pour organiser _prompts_, mémoire et appels d’outils externes dans un flux cohérent [[1]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=Prompt%20chaining%20is%20a%20foundational,answering%20and%20more)[[2]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=LangChain%20provides%20a%20powerful%20framework,LangChain%20handles%20prompt%20chaining%20effectively).

Les agents autonomes récents comme **AutoGPT** étendent cette idée en _auto-gérant une succession de tâches_. AutoGPT (open-source, 2023) utilise GPT-4 pour **découper une consigne complexe en sous-tâches, puis les accomplir séquentiellement** sans intervention humaine[[3]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5). Il s’appuie sur plusieurs instances d’agent coopérant (architecture multi-agents) : l’agent principal élabore un plan, crée des sous-agents spécialisés pour chaque tâche, et coordonne leurs résultats vers l’objectif global[[4]](https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow). Cela permet d’automatiser des workflows entiers qui auraient requis de nombreux _prompts_ manuels
[[3]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5) 
[[4]](https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow). En pratique, AutoGPT intègre des **mémoires** à court et long terme (stockage vectoriel) et des **plugins** (accès Internet, outils tierces) pour enrichir ses capacités[[5]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20works%20by%20processing%20a,real%20time%20to%20iteratively%20improve)[[6]](https://www.ibm.com/think/topics/autogpt#:~:text=In%20addition%20to%20GPT,return%20later%20to%20earlier%20projects). D’autres frameworks multi-agents incluent par exemple _CAMEL_ ou _HuggingGPT_, où des agents LLM collaborent ou se spécialisent (ex. un agent “Manager” délègue à un agent “Solver”). Ces systèmes s’inspirent de principes **cognitifs** : un _agent central_ peut être vu comme un “System 2” (réflexion planifiée) orchestrant des “System 1” plus rapides dédiés à des sous-problèmes.

**Théorie cognitive** : On assimile parfois les LLM à deux modes de pensée (Système 1 vs Système 2 de Kahneman). Un _System 1_ LLM génère une réponse immédiate en une seule passe, tandis qu’un _System 2_ LLM utilise des étapes de raisonnement explicites ou itère sur une question. Par exemple, le _chain-of-thought_ (CoT) est une stratégie System 2 où le modèle, incité à “penser étape par étape”, produit des **raisonnements intermédiaires** avant la réponse finale. Ces méthodes multi-étapes peuvent impliquer plusieurs appels au modèle, des embranchements, des itérations ou des recherches de solution, un peu à la manière de la pensée humaine délibérative. À l’inverse, un modèle en mode direct (System 1) ne considère que l’entrée brute et génère la sortie d’un bloc. Des travaux récents explorent comment **distiller** les performances d’une approche System 2 complexe en un modèle System 1 plus rapide, en le finetunant sur les traces de raisonnement générées par l’approche multi-étapes. Cela vise à combiner le meilleur des deux mondes : l’efficacité en inférence d’un modèle direct, avec la qualité de réponse d’un modèle raisonneur.

Un point central de ces agents est le **mécanisme d’attention et l’émergence de capacités**. Les modèles de langage à base Transformer dotés d’**attention multi-tête** peuvent maintenir un contexte riche et focaliser sur des détails pertinents. La technique du _chain-of-thought_ a révélé que les grands modèles montrent des **capacités émergentes** de raisonnement lorsqu’on leur permet d’écrire leurs étapes de pensée. En d’autres termes, _demander au modèle d’expliquer sa réflexion pas-à-pas_ déclenche des compétences de haut niveau (arithmétique complexe, logique, etc.) **absentes des plus petits modèles**. Ces “capacités émergentes” – c’est-à-dire qui n’apparaissent qu’au-delà d’une certaine échelle de paramètres ou de données – ont été documentées en 2022. Elles suggèrent que l’architecture même (attention, représentations distribuées) favorise l’apparition spontanée de comportements nouveaux avec l’échelle. En agençant l’attention sur une séquence de pensées, les LLM approchent une forme de **méta-cognition** simulée, rappelant l’alternance pensée rapide/pensée lente : la génération immédiate de texte correspond au mode intuitif (_System 1_), tandis que la génération de chaînes de raisonnement explicites se rapproche du mode analytique (_System 2_). Des architectures hybrides cherchent d’ailleurs à intégrer ces deux modes de façon fluide. Par exemple, la méthode **System 2 Attention** (Weston & Sukhbaatar 2023) réalise un premier appel de modèle pour filtrer le contexte et atténuer les biais, suivi d’un second appel pour produire la réponse finale, améliorant la focalisation de l’attention sur les bons éléments. Globalement, l’**émergence** de compétences avec le scaling et le rôle de l’attention dans la gestion du contexte et du raisonnement sont au cœur de la compréhension des performances des agents LLM.

## 2. Prompt engineering avancé : décomposition, meta-prompting et alignment

Le **prompt engineering avancé** consiste à concevoir des consignes sophistiquées pour maximiser les performances des LLM, souvent en explicitant une stratégie de résolution. Une approche phare est la **décomposition de tâche** dans le prompt. La méthode du **Chain-of-Thought (CoT)** l’a popularisée : on ajoute au prompt une instruction du type _“Pense étape par étape”_, ce qui pousse le modèle à **énumérer des étapes intermédiaires** (raisonnements, calculs) avant de donner sa réponse finale. Wei _et al._ (2022) ont montré que cette simple astuce améliorait nettement les résultats sur des problèmes arithmétiques ou logiques complexes, en évitant des erreurs que le modèle aurait faites en répondant d’un seul jet. Depuis, de nombreuses variantes ont vu le jour pour étendre ce principe de décomposition. Par exemple, la **Tree-of-Thoughts (ToT)** introduite par Yao _et al._ (2023-2024) invite le modèle à explorer **plusieurs voies de raisonnement en parallèle** : à chaque étape, le modèle génère plusieurs idées ou actions possibles (branches) plutôt qu’une seule, puis évalue laquelle approfondir. Cela permet de _revenir en arrière_ et d’emprunter une autre branche si une voie s’avère infructueuse – une forme de recherche arborescente dans l’espace des solutions, au lieu d’une simple chaîne linéaire. Plus récemment, la méthode du **Graph-of-Thoughts (GoT)** va encore plus loin en modélisant les pensées du LLM comme un **graphe arbitraire** plutôt qu’une liste ou un arbre [[7]](https://arxiv.org/abs/2308.09687#:~:text=,advantages%20over%20state%20of%20the). Dans ce cadre, chaque unité de pensée (raisonnement partiel, sous-solution) est un nœud, et des arêtes relient les nœuds pour représenter des dépendances ou l’influence d’une idée sur une autre [[7]](https://arxiv.org/abs/2308.09687#:~:text=,advantages%20over%20state%20of%20the). Cette flexibilité autorise des **combinaisons de raisonnements** bien plus riches : le modèle peut fusionner plusieurs idées complémentaires, distiller l’essence d’un réseau entier de pensées, ou renforcer certaines pensées via des boucles de rétroaction [[8]](https://arxiv.org/abs/2308.09687#:~:text=paradigms%20such%20as%20Chain,We%20ensure%20that%20GoT%20is). Les auteurs rapportent que _Graph-of-Thoughts_ améliore la qualité sur des tâches complexes (par ex. un tri difficile voit son taux de réussite augmenter de 62% comparé à Tree-of-Thoughts) tout en réduisant le coût en appels de modèle [[9]](https://arxiv.org/abs/2308.09687#:~:text=approach%20enables%20combining%20arbitrary%20LLM,We%20ensure%20that%20GoT%20is). En somme, CoT, ToT et GoT sont des exemples de **prompting structurel**, où l’on guide le LLM à se comporter comme un solveur explicite – en déroulant une preuve, en explorant un espace de manière arborescente ou en construisant un graphe cognitif.

Une autre dimension du prompt engineering avancé est l’**optimisation du few-shot learning**. Les _few-shot prompts_ fournissent au modèle quelques exemples de la tâche à résoudre, directement inclus dans le prompt. Il a été constaté que la **sélection judicieuse des exemples de démonstration** peut fortement influencer la performance. Des travaux récents proposent d’**automatiser la sélection ou la génération des exemples** plutôt que de les choisir manuellement. Par exemple, certaines méthodes comparent différentes stratégies de sélection (aléatoire, par similarité sémantique, par _clustering_ de l’espace d’embedding, ou via un petit modèle de “compétence” associé) pour voir lesquelles donnent les meilleurs résultats en _in-context learning_. D’autres, face à un _input_ particulier, choisissent dynamiquement les exemples du prompt qui lui ressemblent le plus (_skill-based selection_). On parle aussi de techniques de **recycling** : générer de nouveaux exemples à partir du contexte lui-même – par ex., pour une question de QA sur un long document, extraire une portion différente du document comme exemple supplémentaire, afin d’aider le modèle à mieux répondre. L’optimisation few-shot peut également impliquer le modèle **lui-même dans la boucle** : c’est le concept de **meta-prompting**, où l’on _utilise un LLM pour améliorer ses propres prompts_. En pratique, on peut demander au modèle de proposer plusieurs reformulations de la consigne, ou de générer des exemples de Q/R pertinents, puis de choisir ceux qui semblent maximiser la performance cible. _De Wynter et al._ (2023) formalisent ainsi le meta-prompting comme le fait de _“prompt-to-prompt”_, c’est-à-dire fournir une méta-instruction que le LLM doit suivre pour produire un prompt optimal pour la tâche finale. Ils montrent dans un cadre théorique que le meta-prompting est plus efficace que le prompting de base pour orienter le modèle vers des sorties désirées. Par exemple, plutôt que de résoudre directement un problème, on peut d’abord demander au modèle : _“Quel serait le meilleur plan ou le meilleur prompt pour résoudre ce problème ?”_, puis utiliser sa réponse comme nouveau prompt. Cette approche en deux temps a un effet de **scaffolding** (échafaudage) : le modèle réfléchit d’abord à la stratégie, puis l’applique, ce qui évite d’oublier des étapes cruciales.

Le **prompt chaining** est une technique voisine où l’on enchaîne plusieurs appels successifs au modèle avec des prompts intermédiaires. C’est le principe des **agents conversationnels à étapes multiples** : par exemple un premier prompt sert à analyser la question et extraire des critères, un second prompt utilise ces critères pour interroger une base de connaissances, un troisième formule la réponse finale. Des frameworks comme LangChain facilitent cela en permettant de définir des **chaînes modulaires** où _la sortie d’un modèle devient l’entrée du suivant_, avec éventuellement des branchements conditionnels ou des boucles[[1]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=Prompt%20chaining%20is%20a%20foundational,answering%20and%20more)[[10]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=How%20LangChain%20manages%20prompt%20chaining). Le prompt chaining permet d’aborder les problèmes complexes de façon structurée – on peut ainsi combiner _Récupération + Synthèse + Génération_ en pipeline (voir section Applications). Dans la pratique, ces approches demandent de gérer la **mémoire conversationnelle** (transmettre l’historique aux étapes suivantes) et d’assurer la cohérence globale du système.

Enfin, la question de l’**alignment** (alignement avec les intentions humaines et valeurs) a donné lieu à des techniques de prompt engineering spécifiques, en particulier l’**approche Constitutional AI** d’Anthropic. L’idée est d’introduire dans les prompts un _ensemble de principes directeurs explicites_ – une “constitution” – que le modèle doit suivre pour filtrer ou modifier ses réponses. Plutôt que de recourir uniquement à de la supervision humaine, le modèle est incité à **s’auto-évaluer et s’auto-corriger** selon ces principes éthiques. Concrètement, le processus comporte deux phases : (1) une phase de fine-tuning supervisé où l’on fait générer par le modèle des _auto-critiques_ de ses réponses suivies de versions révisées plus alignées, et on le fine-tune sur ces révisions ; (2) une phase de RL (RLAIF, _Reinforcement Learning from AI Feedback_) où l’on utilise un second modèle pour comparer deux réponses et déterminer laquelle suit le mieux la constitution, afin d’affiner le modèle principal via une récompense. _Bai et al._ (2022) montrent qu’avec une liste d’environ une dizaine de règles (inspirées de principes comme la Déclaration des Droits de l’Homme, la non-malfaisance, etc.), leur modèle Claude peut apprendre à refuser poliment les demandes inappropriées tout en fournissant des explications claires sur ses refus. **Aucune donnée étiquetée manuellement sur des exemples nuisibles n’est nécessaire** – d’où le nom _Constitutional AI_, l’alignement étant piloté par la constitution de règles plutôt que par l’homme directement. En pratique, cela s’implémente souvent via un _prompt système_ contenant la constitution et demandant au modèle de vérifier sa réponse à l’aune de ces principes, puis de la réviser si besoin. On voit ici un cas de _meta-prompting_ appliqué à l’éthique : le modèle endosse tour à tour le rôle de “juré moral” et de “rédacteur” pour s’auto-aligner. Cette technique s’inscrit dans les efforts plus larges pour intégrer l’**alignment** au niveau du prompt et du processus de génération (plutôt qu’uniquement via l’entraînement) – on peut citer aussi les _prompts constitutionnels_ d’OpenAI (règles dans le system message de ChatGPT) ou la _Constitutional Transformer_ de Meta (2023) qui intègre des pénalités dans la fonction de perte pour les violations de contraintes.

## 3. Spécificités selon les modèles LLM (GPT, Claude, Gemini, LLaMA…)

Tous les LLM ne réagissent pas de la même manière aux prompts complexes – en partie à cause de **différences d’architecture et de conception**. Voici un panorama des grandes familles de modèles (2023-2025) et leurs particularités pour le prompt engineering :

- **OpenAI GPT-3.5 / GPT-4** : Les modèles GPT (notamment GPT-4) sont des _Transformers auto-régressifs_ (decodeurs) entraînés sur d’énormes corpus et affinés par _RLHF_ (Reinforcement Learning from Human Feedback). GPT-4 est multimodal (entrée texte **et images**) et a démontré des performances de pointe sur de nombreux benchmarks académiques. Par exemple, GPT-4 atteint un niveau **équivalent au top 10% humain à l’examen du barreau**. Son _contexte maximal_ est d’environ 32k tokens (versions 8k et 32k), ce qui permet des prompts longs et détaillés. En prompt engineering, GPT-4 a montré une forte aptitude à suivre des instructions complexes et à bénéficier de _chain-of-thought_ – OpenAI a même constaté que l’ajout d’un simple “Let’s think step by step” au prompt de GPT-3.5 vs GPT-4 pouvait avoir plus d’effet sur GPT-4, signe qu’il exploite mieux cette indication. GPT utilise un système de _messages hiérarchiques_ (system, user, assistant) où le **message système** sert à guider globalement le comportement (ton, style, règles). Les ingénieurs s’en servent pour injecter des **méta-instructions** d’alignement ou de format de réponse. GPT-4 étant fermé et propriétaire, on ne connaît pas précisément son architecture interne ni son nombre de paramètres, mais OpenAI a souligné l’effort mis dans l’**optimisation de l’alignement** (avec des techniques de pré-entraînement prédictif sur 1/1000ème de sa taille pour prévoir ses capacités et éviter les échecs d’échelle). En pratique, GPT-4 est très sensible au phrasé du prompt : la même demande formulée différemment peut donner des réponses plus ou moins pertinentes. Les _prompts constitutionnels_ ou _règles de système_ fonctionnent bien pour brider GPT-4, mais il reste vulnérable aux _prompt injections_ habiles. Aussi, GPT-4 a tendance à suivre _strictement le style demandé_ – ce qui est un atout (on peut demander “réponds sous forme de JSON” et il s’y tiendra assez fidèlement, mieux que GPT-3.5).
- **Anthropic Claude 2** : Claude est l’assistant d’Anthropic, construit avec un accent sur la **sécurité et l’explicabilité**. Il a été entraîné via le procédé _Constitutional AI_ évoqué plus haut, ce qui se ressent dans ses réponses souvent plus **verbeuses et nuancées** que GPT. Claude excelle à maintenir de longues conversations grâce à son **énorme fenêtre de contexte (100k tokens)** dès Claude 2 (2023), portée à _200k tokens_ avec Claude 2.1 fin 2023. Cela représente des centaines de pages de texte en mémoire, idéal pour du _retrieval augmenté_ ou l’analyse de longs documents. En termes de prompt, Claude supporte bien les **prompts très longs** (par ex. lui fournir tout un manuel en contexte) et a été _spécifiquement entraîné à la fiabilité sur longue distance_ : Anthropic rapporte que Claude 2.1 fait 30% d’erreurs en moins que Claude 2.0 dans des tâches de question sur documents longs, grâce à un entraînement ciblé sur ce cas. Cependant, cette orientation sécurité fait que Claude est parfois **rétiçent à certaines consignes** : par exemple, des tests ont montré qu’il refusait de répondre à une question si l’information nécessaire était présente _mais noyée_ au milieu d’un document de 200k tokens, craignant peut-être un piège contextuel. En ajustant le prompt (en ajoutant une phrase du type “Voici la phrase la plus pertinente : ...” pour le rassurer), on peut lever cette hésitation. Claude est donc sensible aux _indices de confiance_ qu’on lui donne dans le prompt. Par ailleurs, il gère très bien les instructions de formatage (JSON, YAML, etc.) – Anthropic indique qu’il a été entraîné à produire des sorties dans des formats structurés sur demande. En comparaison de GPT-4, Claude est perçu comme **moins bridé sur la longueur** (il peut produire des réponses plus longues et détaillées facilement) et parfois meilleur en maintien du contexte conversationnel (moins de pertes de fil sur de longues sessions). En prompt engineering, Claude supporte naturellement l’instruction “pensée en chaîne” (il produit volontiers des raisonnements intermédiaires si on le lui demande, sans devoir l’y forcer). Un aspect distinctif est que Claude est **moins sensible au ton du system prompt** : ayant été entraîné avec une constitution morale, même si on ne précise rien, il évitera les contenus non alignés. Cela peut le rendre _plus robuste aux attaques par prompt_ (il a été noté que Claude résiste un peu mieux aux _jailbreaks_, bien que ce ne soit pas infaillible). Enfin, Claude est actuellement disponible jusqu’en version 2.1 (2024) et Anthropic planche sur Claude 3 ; l’évolution devrait porter sur encore plus de contexte et une meilleure capacité multimodale possiblement.

- **Google DeepMind Gemini** : Annoncé en fin 2023, **Gemini** est la réponse de Google à GPT-4, développé après la fusion avec DeepMind. C’est une famille de modèles _multimodaux_ conçus pour être _généraux et efficaces_. Gemini est décliné en plusieurs tailles (Ultra, Pro, Nano) pour différents usages. **Gemini Ultra**, le plus grand, a établi de nouveaux records début 2024 en surpassant l’état de l’art sur 30 des 32 principaux benchmarks académiques évalués[[11]](https://blog.google/technology/ai/google-gemini-ai/#:~:text=match%20at%20L478%20We%27ve%20been,academic%20benchmarks%20used%20in%20large). Notamment, c’est le **premier modèle à dépasser la performance humaine moyenne sur MMLU** (un ensemble de 57 matières de connaissances universitaires) avec un score de 90%. Gemini excelle en **raisonnement mathématique et scientifique** et obtient 59.4% sur un benchmark multimodal difficile (MMMU), mieux que tous ses prédécesseurs. Il brille aussi en vision : sur des tests images, Gemini Ultra bat les anciens modèles tout en se passant d’OCR externe pour lire le texte dans les images. Pour le prompt engineering, cela signifie que Gemini peut ingérer non seulement du texte mais aussi des images (voire audio/vidéo selon Google) en contexte, permettant des _prompts multi-modaux_ du type : _“Voici une image, que se passe-t-il ?”_. Sa **capacité de planification** a été vantée par Demis Hassabis, suggérant une intégration de techniques de renforcement et peut-être d’arborescence de pensée dans son utilisation. Google a aussi insisté sur l’**efficacité** de Gemini : il est conçu pour bien s’adapter à différents déploiements (du cloud jusqu’au mobile), donc son architecture a certainement été optimisée pour différentes échelles. On peut supposer que Gemini utilise des idées de PaLM 2, de LaMDA (dialogue) et des avancées de DeepMind en RL. En tout cas, lors de son lancement, un accent a été mis sur les **capacités de raisonnement améliorées** – par ex., Google mentionne un nouveau mode d’utilisation de MMLU où le modèle “réfléchit plus posément avant de répondre”, améliorant son score. Côté alignement, on sait peu de choses précises, mais Google a évoqué l’emploi de techniques d’évitement des hallucinations et de filtrage via des instructions de sécurité (probablement dans le style des _policy prompts_ de leur PaLM 2). Pour un ingénieur, tirer le plein potentiel de Gemini implique d’exploiter sa multimodalité (fournir schémas, images explicatives dans le prompt si possible) et sa capacité à **gérer des instructions complexes** (compte tenu de ses performances, il peut sans doute suivre de longues listes d’instructions ou gérer des tâches à étapes multiples sans trop guider).

- **Meta LLaMA (v2, v3)** : Meta AI a adopté une approche open-source avec la famille **LLaMA**. LLaMA 1 (février 2023) et LLaMA 2 (juillet 2023) sont des modèles disponibles en poids bruts, de 7B à 70B paramètres, qui ont permis à la communauté de réaliser des _fine-tunes_ spécialisés. LLaMA 2 a été entraîné en suivant la recette _Chinchilla_ (rapport données/paramètres optimisé) et s’est révélé très performant pour son échelle, approchant le niveau de GPT-3.5 sur certaines tâches tout en étant open-source. En 2024, **LLaMA 3** a été introduit, avec une version **3.1** qui pousse l’ouverture à son comble : Meta a sorti un modèle colossal de **405 milliards de paramètres**, le plus grand modèle de base ouvert connu. Ce LLaMA 3.1 405B est présenté comme _“le modèle fondationnel ouvert le plus capable à ce jour”_, prétendument **comparable à GPT-4 sur de nombreux points** d’après Meta (qui avance même qu’il rivalise avec GPT-4 sur la compréhension générale et dépasse GPT-4 sur certaines tâches multimodales, selon un article de presse). Une telle taille pose des défis techniques (coût d’inférence, latence) mais ouvre la voie à des fine-tunes très puissants accessibles aux chercheurs. Pour le prompt engineering, les LLaMA présentent quelques particularités : _par défaut_, leur entraînement n’inclut pas de format conversationnel avec rôles comme ChatGPT, donc on doit souvent **instruire explicitement le style** (sauf si on utilise une version fine-tunée instruct). Les modèles LLaMA 2-chat et suivants toutefois sont _instruction-tuned_ avec des données de dialogue, donc ils comprennent les indications utilisateur classiques. Ils ont en général une **fenêtre de contexte plus petite** que GPT/Claude (2048 tokens de base pour LLaMA 2, même si des versions élargies à 8k ou 16k existent via fine-tune). Leurs réponses peuvent être un peu plus littérales et moins filtrées niveau sécurité (puisque l’alignement RLHF sur LLaMA 2-chat était limité, et inexistant sur LLaMA de base). Cela signifie qu’un prompt mal contrôlé sur LLaMA open peut facilement produire du contenu non aligné ou déraper – l’utilisateur doit donc inclure des instructions de modération si nécessaire. En contrepartie, l’absence de garde-fous stricts rend LLaMA flexible pour des utilisations créatives ou techniques (pas de refus sur du code potentiellement malveillant par ex., là où GPT-4 s’abstiendrait). Avec LLaMA 3, Meta introduit possiblement des **améliorations d’architecture** (on parle de “Giraffe” pour permettre plus de contexte, ou de nouvelles optimisations mémoire). Une force de l’écosystème LLaMA est la **fine-tune spécialisée** : par prompt engineering, on peut combiner un modèle LLaMA avec un _LoRA fine-tune_ particulier pour une tâche. Par exemple, pour du raisonnement math, utiliser un modèle fine-tuné sur du CoT math (comme Mistral ou WizardMath) améliorera nettement les réponses étape-par-étape. En résumé, LLaMA est un choix privilégié des ingénieurs _open-source_ qui veulent contrôler et adapter le modèle ; son prompt engineering dépend plus fortement de l’exactitude de la formulation (il pardonne moins les instructions ambiguës que GPT-4) et de la possible adaptation du modèle en amont (via fine-tune) pour certains formats de prompt.

**Benchmarks comparatifs** : Sur des évaluations standard fin 2024, GPT-4 reste généralement le plus performant _généraliste_, en particulier en créativité et code. Claude 2 est proche derrière en compréhension et fait mieux sur la longueur de contexte. Gemini Ultra semble désormais en tête sur des tâches académiques structurées (MMLU, etc.), indiquant une avance en connaissance factuelle et en raisonnement multi-modal. LLaMA 3.1 405B, quant à lui, aurait des scores proches de GPT-4 dans divers domaines d’après Meta (par ex. ils annoncent rivaliser sur _Hellaswag, GSM8K_ etc. pour la version 70B, et le 405B vise à combler tout écart). Un choix d’ingénierie courant est de combiner **plusieurs modèles** pour tirer parti de leurs forces respectives via un orchestrateur (ex : utiliser GPT-4 pour vérifier ou améliorer la réponse d’un modèle open, utiliser Claude pour résumer une longue entrée puis GPT-4 pour affiner la réponse finale, etc.). Chaque modèle a aussi ses _sensibilités_ : par exemple GPT-3.5 est connu pour parfois “délirer” s’il n’est pas guidé précisément (il hallucine plus), alors que GPT-4 est plus sobre. Claude aime bien quand on lui donne un rôle ou une motivation (il répondra très longuement si on lui dit “explique en détail”), et reste poli en toutes circonstances (grâce à sa constitution). Les modèles open plus légers (7B, 13B) nécessitent souvent des prompts _très détaillés_ pour compenser leur manque de connaissances ou de capacité de déduction – d’où l’émergence de _prompt templates_ communautaires (“WizardLM style”, “Alpaca format”, etc.) optimisés pour eux.

## 4. État de l’art académique (2023-2025) : interprétabilité, scaling laws, robustesse

La recherche académique récente a été foisonnante autour des LLM, avec de nombreuses publications dans NeurIPS, ICML, ICLR, ACL, EMNLP, etc. **Plusieurs tendances clés** se dessinent :

- **Mécanismes de raisonnement et agents LLM** : Des papiers comme _ReAct_ (Yao et al., 2022) ont jeté les bases de l’agentité en LLM, en montrant qu’un LLM pré-entraîné peut servir d’**agent généraliste** si on le _prompt_ correctement pour alterner pensée et action. Par la suite, _Tree-of-Thoughts_ (Yao et al., 2023) a été publié dans _NeurIPS 2023_, démontrant qu’on peut améliorer sensiblement la résolution de problèmes complexes en faisant **explorer plusieurs solutions simultanément** plutôt qu’une seule (avec un arbre explicite de réflexions évaluées). Cette approche a ouvert la voie à des variantes comme _Graph-of-Thoughts_ (Besta et al., _AAAI 2024_)[[7]](https://arxiv.org/abs/2308.09687#:~:text=,advantages%20over%20state%20of%20the), évoquée plus haut, qui propose un cadre unificateur englobant CoT et ToT. On voit également apparaître des méthodes telles que **Branch-Solve-Merge** (Saha et al., 2023) où le modèle génère un plan, _branche_ le problème en sous-problèmes, résout chaque branche avec un LLM, puis _fusionne_ les résultats[[12]](https://arxiv.org/html/2407.06023v1#:~:text=the%20latter%20to%20expand%20on,final%20stage%20merges%20the%20results). Cette idée de faire du _divide-and-conquer_ guidé par le LLM lui-même est un thème fréquent. D’autres travaux (e.g. Weston & Sukhbaatar 2023) explorent la notion d’**attention de haut niveau** (“System 2 Attention”) en deux passes : la première passe sert à survoler la question et repérer les éléments saillants (et éventuellement reformuler la question pour éliminer les ambiguités), la seconde passe produit la réponse détaillée. L’objectif est d’atténuer les erreurs dues à l’attention diffuse d’un seul passage, et cela s’apparente à doter le modèle d’une étape de “concentration” avant de répondre. Globalement, l’**idée d’itérer avec le modèle** sur la question (via plusieurs _prompts_ ou une conversation interne) a le vent en poupe, car elle compense les limites d’une génération directe. Une preuve en est l’essor des _self-reflection_ methods comme **Self-Refine/Reflect** (Shinn et al., 2023, ou Shumailov et al., 2023) où le modèle génère d’abord une réponse, puis _critique sa propre réponse_ et la régénère améliorée. De même, **DeepMind’s Reflexion** (Noorulai et al., 2023) fait mémoriser par l’agent ses échecs passés pour éviter de les répéter, ce qui amène un comportement émergent de type « apprentissage par essai-erreur » en cours d’inférence.
- **Interprétabilité des LLM** : Face au caractère _boîte noire_ de ces gigantesques réseaux, de nombreux chercheurs tentent de comprendre **comment** ils traitent l’information et arrivent à des solutions. Une direction active est la **mécanistic interpretability** adaptée aux Transformers. Par exemple, des études analysent le rôle des **têtes d’attention** individuelles dans GPT-2/GPT-3 : il a été montré que certaines têtes implémentent des fonctions précises (suivi de chaîne de conversation, gestion des guillemets, etc.), et des travaux de 2023 recensent systématiquement ces comportements d’attention. Une _review_ de 2023 dans _Trans. on ML Research_ souligne qu’il existe des têtes d’attention spécialisées qui se comportent de manière interprétable dans divers scénarios (copie de séquence, détection de token de clôture, etc.). Au-delà des têtes, on examine les **neurones individuels** et leurs activations : OpenAI a publié en 2022 une analyse nommée “**Toy Models of Neurons in LLMs**” où ils cherchaient à associer des neurones à des concepts humains compréhensibles (par ex. un neurone qui s’active sur les noms d’animaux). En 2023, la technique de la **“logit lens”** (insérer un décoder sur les états intermédiaires pour voir quel mot serait prédit si on coupait le modèle à cette couche) a été utilisée pour visualiser comment la prédiction évolue couche par couche. Des chercheurs de Stanford ont également questionné la notion de _chain-of-thought interne_ : par exemple, _Meng et al. (2022)_ ont cherché des “moniteurs de boucle de pensée” dans le réseau – sans conclusions définitives, mais ouvrant la voie à insérer peut-être des _raccourcis interprétables_ dans le modèle. Une avancée notable est l’outil **Tracr** de DeepMind (2022-23) qui a entraîné un Transformer miniature à exécuter du code Python, puis a réussi à **tracer parfaitement l’exécution** à travers le réseau (chaque neurone ayant un rôle clair dans l’exécution). Cela reste sur des cas jouets, mais cela prouve qu’on peut, au moins pour de petits modèles, atteindre une interprétabilité de type “lecture du code” complet. En 2024, la tendance est de combiner cette approche avec des LLM _assistants eux-mêmes_ : on voit poindre des idées où un LLM commente les activations d’un autre (ou de lui-même) pour expliquer ce qu’il fait – une sorte de _self-explanation_. Aussi, la **transparence des raisonnements** a été abordée via l’**apprentissage de rationales** : Anthropic a montré dans Constitutional AI que forcer le modèle à fournir une _justification style chain-of-thought de ses réponses_ pouvait améliorer la _transparence et l’acceptabilité_ aux yeux humains. Enfin, certaines conférences (ICLR 2024 workshops) débattent de la question : _les chaînes de pensée générées sont-elles vraiment représentatives de la façon dont le modèle raisonne en interne, ou juste un artifice ?_ Des papiers comme _Mirage of Emergent Abilities (2023)_ suggèrent que ce qu’on appelle “émergent” peut parfois être un artefact des métriques choisis plutôt qu’une véritable discontinuité de capacité – d’où l’importance d’outils analytiques plus fins pour saisir la transition de comportement avec l’échelle.

- **Scaling laws & robustesse** : L’étude des _lois d’échelle_ des LLM (combien de données et de paramètres pour quelles performances) a été revitalisée par les découvertes de _Chinchilla_ (Hoffmann et al., 2022) montrant qu’on peut obtenir un modèle meilleur qu’un GPT-3 175B en utilisant 4× moins de paramètres (70B) mais 4× plus de données – ce qui a mené à LLaMA, etc. En 2023, des travaux ont cherché à affiner ces lois sur de nouveaux axes, notamment la **robustesse**. Une question posée : _augmente-t-elle avec la taille du modèle ?_ Intuitivement, on pourrait penser qu’un plus gros modèle, entraîné sur plus de données, généralise mieux et serait plus robuste aux perturbations ou aux attaques adverses. Or, **les résultats sont mitigés**. Une étude (_Scaling Trends in Language Model Robustness_, ICLR 2024) conclut que **sans entraînement de sécurité explicite, les modèles plus grands ne sont pas systématiquement plus robustes** que les plus petits face à des inputs adversariaux ou du bruit. Ils observent cependant que l’augmentation de l’échelle **améliore la robustesse en moyenne** sur des perturbations mineures (_sample efficiency_ plus élevée), mais introduit de nouvelles faiblesses qualitatives. Par exemple, un GPT-2 (petit) et un GPT-3 (grand) peuvent tous deux échouer sur un certain casse-tête logique ; mais GPT-3 va parfois donner une réponse extrêmement confiante et sophistiquée mais fausse (hallucination élaborée), là où GPT-2 avouera plus facilement ne pas savoir. Donc la **nature des échecs évolue** avec la taille. Une autre direction de scaling law en 2023 est l’étude du _dilemme taille vs. coût_ : sachant qu’on peut distiller ou quantifier des modèles, certaines recherches (de EffCommAI) ont montré qu’un modèle 13B bien fine-tuné peut accomplir 90% du travail d’un 65B sur des tâches instruct, pour un coût bien moindre – d’où des réflexions sur la “taille effective” après fine-tuning. Par ailleurs, la robustesse face aux attaques de prompt (jailbreaks) est devenue un benchmark à part : Anthropic dans son _Claude 2_ whitepaper mentionne tester des centaines de _prompt d’attaque connus_ et mesurer combien passent au travers. Là encore, ce n’est pas simplement une question de taille : c’est plus lié aux _techniques d’alignement_ et aux _contre-mesures_ (comme des classifieurs embarqués). D’autres “scaling laws” intéressantes portent sur la **mémoire à long terme** : des travaux explorent comment la capacité à rappeler des faits lointains évolue avec la taille du contexte et du modèle – on voit que même avec 100k tokens, le modèle peut perdre le fil sans aide (d’où l’idée de _re-injecter des rappels_ régulièrement, ou d’entraîner des _recall heads_ spécialisées). Enfin, un aspect critique de robustesse est la **résistance aux changements de distribution** (out-of-distribution performance) : des études en 2023 ont suggéré qu’au delà d’une certaine échelle, les modèles deviennent étonnamment bons pour se _reparamétrer_ sur de nouvelles distributions avec très peu d’exemples (d’où le succès de l’in-context learning). Cependant, ils restent vulnérables à des perturbations subtiles qu’ils n’ont jamais vues (par ex. une question triviale posée dans un style inhabituel). Cela alimente la recherche sur des entraînements plus diversifiés (_Mixture of Styles_, _Noisy Context Training_).

En résumé, l’état de l’art académique fin 2024 témoigne d’une **maturation** : on ne cherche plus seulement à battre des records de taille, mais à _mieux utiliser_ les modèles existants (via distillation, combinaison de méthodes, etc.), à **comprendre leurs limites internes** et à **combler les faiblesses** (hallucinations, biais, non-robustesse) par des moyens algorithmiques. La synergie entre recherches académiques et industrielles est forte – beaucoup de papiers influents viennent de laboratoires privés (OpenAI, Anthropic, DeepMind, Meta) et sont ensuite discutés en conférences. On voit aussi émerger des **outils open-source d’évaluation** (HELM de Stanford, Open LLM Leaderboard) qui uniformisent les comparaisons. L’**interprétabilité** reste en grande partie un défi ouvert, mais les efforts en cours (analyser les circuits neuronaux des LLM) rappellent ceux faits jadis pour le vision _deep learning_, avec l’espoir de découvrir des “unités sémantiques” ou des “sous-modules” dans ces réseaux géants.

## 5. Applications pratiques émergentes : RAG, agents autonomes et multimodalité

Les progrès en systèmes agentiques et en prompting se concrétisent dans des applications de plus en plus complexes :

- **Retrieval-Augmented Generation (RAG) avancé** : Le principe de base du RAG est d’**augmenter le contexte du LLM avec des informations externes pertinentes** (documents, base de connaissances) afin de pallier ses lacunes – par exemple son savoir figé ou ses hallucinations[[13]](https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a). En 2023, le RAG est devenu quasi standard pour toute application nécessitant des connaissances à jour (chatbots documentaires, assistants métier, etc.). On voit maintenant des _pipelines RAG_ sophistiqués : plutôt qu’une simple recherche naïve de quelques documents, les systèmes intègrent des étapes de _pré- et post-traitement optimisés_. Par exemple, la stratégie **“retrieve then read then refine”** : le LLM formule d’abord une requête de recherche (grâce à un prompt intermédiaire), un moteur de vectorisation renvoie des passages, le LLM génère une réponse à partir de ces passages, puis dans un second temps on le reprompt pour vérifier et citer précisément les sources. Des frameworks comme **LlamaIndex (GPT Index)** permettent de chaîner ces opérations simplement. Une revue de fin 2023 distingue le _Naive RAG_ (recherche directe + concaténation au prompt) de l’**Advanced RAG** qui inclut des étapes additionnelles (filtrage de documents, synthèse multi-doc, etc.) et du _Modular RAG_ où les composants recherche/génération/combinaison sont clairement séparés et optimisés indépendamment[[14]](https://arxiv.org/abs/2312.10997#:~:text=synergistically%20merges%20LLMs%27%20intrinsic%20knowledge,faced%20and%20points%20out%20prospective). Les techniques d’**augmentation du prompt** incluent aussi la **récupération itérative** (poser des questions de clarification en plusieurs tours pour affiner la recherche) et l’**intégration de connaissances structurées** (par ex, récupérer non seulement du texte libre mais aussi des entrées de base de données, des calculs via un outil, etc., et tout fournir de façon formatée dans le prompt). L’impact est majeur sur la **fiabilité** : un LLM bien couplé à un bon module de recherche hallucine beaucoup moins et peut être mis à jour facilement en actualisant l’index documentaire sans retrainer le modèle[[13]](https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a). Toutefois, cela ajoute des défis : comment faire citer correctement la source (d’où les méthodes de _prompting_ spécialement conçues pour faire dire “selon le document X : …”), comment éviter que le LLM n’utilise sa _mémoire interne_ contradictoire à la vérité récupérée, etc. Des méthodes comme **“knowledge injection prompts”** obligent le modèle à utiliser exclusivement le contenu fourni – en disant par ex : _“Si l’information n’est pas dans les sources ci-dessous, réponds ‘je ne sais pas’.”_. Des benchmarks dédiés (_LAMBADA-open_ etc.) mesurent les gains de RAG. En pratique, le RAG avancé est utilisé dans les chatbots genre **Bing Chat** ou **ChatGPT Plugins** (lequel utilise le web browsing en direct) pour allier la puissance générative du LLM et la **factualité à jour** du moteur de recherche[[13]](https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a)[[15]](https://arxiv.org/abs/2312.10997#:~:text=tripartite%20foundation%20of%20RAG%20frameworks%2C,avenues%20for%20research%20and%20development).

- **Agents autonomes** : Inspirés par AutoGPT et consorts, de plus en plus d’applications tentent d’utiliser un LLM comme **agent complet** réalisant une tâche complexe de bout en bout. Par exemple, des projets comme **GPT-Engineer** ou **Meta’s ChatDev** utilisent des LLM pour générer du code via une série d’étapes planifiées (écrire le plan de projet, générer le code module par module, s’auto-vérifier, etc.). Dans le jeu vidéo, l’agent **Voyager (Wang et al., 2023)** a été un précurseur : c’est un agent GPT-4 autonome qui explore _Minecraft_ en se fixant des objectifs, en apprenant de nouvelles compétences (stockées dans un code Python qu’il modifie lui-même) – il a réussi à découvrir des outils du jeu de façon non supervisée, montrant la puissance d’un LLM couplé à une mémoire externe persistante. En simulation sociale, _Generative Agents (Park et al., 2023)_ a fait sensation : ils ont créé un petit monde virtuel (façon Sims) où chaque personnage est un agent piloté par un LLM ChatGPT, avec une mémoire propre et des routines quotidiennes. Ces agents interagissent entre eux de manière cohérente, planifient leur journée (aller travailler, organiser une fête) – tout cela _émerge_ simplement de la configuration initiale et de prompts décrivant leur personnalité et leurs souvenirs. On touche là à la création de **comportements réalistes sur le long terme**, une frontière entre NLP et IA cognitive. D’un point de vue technique, ces agents combinent _mémoire à long terme_ (base de données de faits que l’agent peut interroger, par ex via un vecteur embedding de ses souvenirs) et _chaînes de réflexion planificatrices_. Un défi est d’éviter la dérive du comportement au fil du temps (d’où l’intérêt de reseter ou de résumer la mémoire régulièrement). Sur le plan pratique, on voit apparaître des **systèmes d’orchestration d’agents** : par ex. le framework **LangChain** propose des _Agent Executors_ qui, donnés une liste d’outils utilisables (navigateur web, calculatrice, API spécifiques), vont formuler des plans du type _“J’ai besoin de faire X, donc d’abord utiliser la calculatrice, puis chercher sur le web…”_. C’est typiquement implémenté avec un prompt contenant un format de réflexion (par ex. la **Prompt ReAct** style : _Thought: … / Action: … / Observation: … / Thought: … / Action: …_). Des agents spécialisés (récemment **Ghostwriter** pour écrire du contenu marketing, ou **JARVIS** pour la programmation assistée) montrent qu’on peut atteindre une autonomie partielle dans certaines tâches professionnelles. Cependant, l’autonomie complète demeure délicate : les retours d’expérience sur AutoGPT indiquaient qu’il “tournait parfois en rond” ou prenait des décisions absurdes sans supervision humaine. Ainsi, la pratique courante est de garder un **humain dans la boucle**, au moins pour valider les étapes-clés ou fournir des _hints_ si l’agent bloque. Néanmoins, l’**amélioration rapide des modèles** (GPT-4.5, GPT-5 potentiellement) et l’**affinage des techniques de mémoire** laissent penser que les agents auto-gérés gagneront en fiabilité au cours de 2024-2025.

- **Intégration d’outils externes** : Un aspect crucial des agents est la capacité à utiliser des **outils** – que ce soit appeler une API, exécuter du code, interagir avec une base de données ou contrôler un robot. Des travaux comme **Toolformer** (Schick et al., 2023) ont démontré qu’un LLM peut **s’auto-apprendre** à insérer des appels d’API dans sa génération pour pallier ses faiblesses. Dans Toolformer, on fournit au modèle quelques exemples annotés de texte où certaines parties ont été obtenues via un outil (par ex “=calc(42_19)=798”), puis on le laisse générer sur de grandes données en détectant_ quand _un appel d’API serait utile (en se basant sur la probabilité de suite de texte) – ce processus pseudo-auto-supervisé a permis de doter un modèle de la capacité de_ _faire du calcul arithmétique exact en appelant une calculatrice, de_ _traduire via une API de traduction_ _quand c’est plus fiable, ou de_ _faire une recherche web_ _pour des faits inconnus. Le résultat, c’est un modèle qui combine le meilleur des deux mondes : la souplesse du langage et la fiabilité des outils spécialisés. Cette idée a fortement inspiré l’industrie : OpenAI a sorti en 2023 la fonction_ Function Calling _pour ses modèles, permettant au LLM de renvoyer un JSON indiquant quel outil appeler avec quels arguments. Microsoft a développé_ _HuggingGPT_ _(Liang et al., 2023) où un LLM pilote une panoplie de modèles experts (vision, audio…) en orchestrant leurs appels selon la demande utilisateur. Par exemple, on demande_ “Décris-moi cette image et lis-moi le texte qui s’y trouve”_, le LLM va d’abord appeler un modèle OCR pour le texte, un modèle de description d’image, puis combiner les résultats dans une réponse finale. Tout cela se fait via du prompt engineering où l’on présente la liste des outils disponibles et le format attendu. Dans les applications pratiques, on voit exploser les cas d’usage :_ _agents de navigation web_ _(le LLM appelle l’outil “Browser” pour faire une recherche internet en direct),_ _agents code runner_ _(le LLM peut exécuter du code Python sur une sandbox via un appel – utile pour calculer une réponse exacte ou tester une idée),_ _agents base de données_ _(il peut formuler une requête SQL et l’exécuter pour obtenir des données précises). Un bénéfice immédiat est la fiabilité accrue : plutôt que de laisser le LLM halluciner un résultat de calcul, on préfère qu’il appelle la calculatrice. Cela nécessite que le LLM apprenne_ quand _déléguer – c’est souvent géré par un_ prompt spéciale _du style :_ “Si la question requiert un calcul, tu dois utiliser l’outil Calculator de la façon suivante…”_. Avec GPT-4, on a vu qu’il apprend très bien ce protocole avec quelques exemples. Les agents outillés posent aussi la question de la_ _sécurité_ _: donner à un LLM l’accès à des outils potentiellement dangereux (exécuter du code système, envoyer des emails…) doit être fait prudemment, avec des sandboxes et des garde-fous sur les prompts. Néanmoins, c’est clairement la direction prise pour rendre les LLM_ _plus utiles et connectés au monde réel_*.

- **Systèmes multi-modaux** : L’intégration de diverses modalités (texte, vision, audio, etc.) est un grand axe de 2023-2025. GPT-4 a introduit l’entrée d’images, ouvrant la voie à des use cases comme décrire une image, analyser un graphique, résoudre des problèmes visuels. D’autres modèles multi-modaux ont émergé : **CLIP-GPT** et **BLIP-2** combinent des encodeurs d’images avec des LLM pour permettre la _vision2text_ via prompt. Google a mentionné que **Gemini** sera nativement multi-modal, pouvant traiter images, texte et peut-être audio. Meta a sorti **ImageBind** et **SAM** pour la vision, et on peut imaginer un LLaMA futur couplé à ces modules. En pratique, le prompt engineering multimodal consiste à insérer des **descriptions de la modalité** ou des espaces réservés dans le prompt : par exemple _“<image>…</image>”_ avec une description ou un ID d’image. Dans GPT-4 vision, l’utilisateur peut réellement attacher l’image et poser une question, ce qui derrière les coulisses fait qu’un module visuel encode l’image en embeddings compatibles que le LLM utilise. Un défi de ces systèmes est la longueur et le caractère peu structuré des données visuelles comparé au texte – d’où l’importance d’**orienter le modèle sur quoi regarder**. On a vu par ex. des prompts du style _“Regarde l’image jointe. Il s’agit d’un graphique linéaire. Que représente l’axe X ?”_ pour aider GPT-4 à focaliser sur la légende du graphique. D’autres systèmes multi-modaux comprennent **VLMs (Vision-Language Models)** comme _Flamingo_ (DeepMind) ou _LLaVA_ (Large Language and Vision Assistant, 2023) qui finetune un LLM avec un corpus d’images décrites, pour le rendre capable de dialogue visuel. On voit aussi l’audio : OpenAI **Whisper** convertit la parole en texte, qui peut ensuite être traité par ChatGPT – déjà des intégrations permettent de parler à ChatGPT oralement. Le futur proche verra plus de **fusion des modalités dans les prompts** : par ex, “Voici une photo du plan de l’appartement et une consigne textuelle, génère-moi un aménagement 3D optimal” – l’agent devra comprendre l’image (plan), le texte, et éventuellement produire une autre modalité (un modèle 3D ou un dessin, via un outil). Des approches comme **Generative Fill** (Adobe Firefly) combinent déjà LLM et modèle génératif d’image : l’utilisateur donne une directive textuelle sur une image, le LLM interprète, puis un modèle image exécute. Tout cela sera de plus en plus orchestré via des _meta-prompts_ qui coordonnent plusieurs modèles multi-modaux.

**Tendances émergentes** : On observe une convergence entre **monde académique et outils pratiques**. Les idées de la recherche (CoT, RAG, Toolformer, etc.) sont rapidement implémentées dans des frameworks (Haystack, LangChain, LlamaIndex...). Réciproquement, de nouveaux besoins industriels (comme _assurance qualité des LLM_, _réduction des coûts_) font naître des travaux académiques en efficacité (distillation, compression 4-bit, serveurs spécialisés). Un enjeu à l’horizon est la **personnalisation locale** : avoir son propre LLM aligné à ses données et préférences. Des techniques de _Low-Rank Adaptation (LoRA)_ ou de _préférence learning_ simplifié pourront permettre à un ingénieur ML de “spécialiser” un agent sans repartir de zéro. Enfin, la question du **trade-off complexité vs performance** se pose toujours : des promptings très complexes (multi-étapes, multi-agents) donnent des résultats impressionnants, mais au prix d’une latence et d’un coût élevés (multiplication des appels d’API). Trouver l’équilibre entre _prompt simple + modèle gros_ ou _prompt élaboré + modèle plus petit_ est un art en soi. Par exemple, une chaîne de 10 appels avec un modèle open 7B peut parfois accomplir la même tâche qu’un seul appel à GPT-4 – mais lequel est optimal en pratique dépend du contexte (coût, données sensibles, etc.). L’état de l’art tend à montrer que _davantage de structure et de supervision dans le processus de génération_ améliore la fiabilité, au prix d’efforts d’ingénierie. On peut s’attendre à ce que les futurs LLM intègrent nativement certaines de ces avancées (ex : un GPT-5 qui aurait en interne un mécanisme de CoT ou un module de recherche web). En attendant, la combinaison ingénieuse de ces pièces via du prompt engineering et des systèmes agentiques reste le terrain de jeu privilégié des ingénieurs ML en 2025, promettant des systèmes d’IA **toujours plus autonomes, précis et polyvalents**.

**Références :**

·      Yao et al. “ReAct: Synergizing Reasoning and Acting in Language Models” (2022)

·      Bai et al. “Constitutional AI: Harmlessness from AI Feedback” (Anthropic, 2022)

·      Weston et al. “System 2 Attention” (Meta, 2023) – _ICLR 2024_.

·      Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools” (2023)

·      Yao et al. “Tree of Thoughts: Deliberate Problem Solving with LLMs” (NeurIPS 2023).

·      Besta et al. “Graph of Thoughts: Solving Elaborate Problems with LLMs” (AAAI 2024)[[7]](https://arxiv.org/abs/2308.09687#:~:text=,advantages%20over%20state%20of%20the)[[16]](https://arxiv.org/abs/2308.09687#:~:text=approach%20enables%20combining%20arbitrary%20LLM,both%20of%20which%20form%20complex).

·      OpenAI “GPT-4 Technical Report” (2023).

·      Anthropic “Claude 2 Launch” (2023).

·      Google DeepMind “Introducing Gemini” (blog, 2023)[[11]](https://blog.google/technology/ai/google-gemini-ai/#:~:text=match%20at%20L478%20We%27ve%20been,academic%20benchmarks%20used%20in%20large).

·      IBM Prompt Chaining & AutoGPT Guides (2023)[[3]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5)[[4]](https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow)[[1]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=Prompt%20chaining%20is%20a%20foundational,answering%20and%20more).

·      Gao et al. “Retrieval-Augmented Generation for LLMs: A Survey” (2024)[[13]](https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a)[[14]](https://arxiv.org/abs/2312.10997#:~:text=synergistically%20merges%20LLMs%27%20intrinsic%20knowledge,faced%20and%20points%20out%20prospective).

·      _Et de nombreuses autres sources académiques 2023-2024 sur l’alignement, la robustesse et l’interprétabilité des LLM._

---

[[1]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=Prompt%20chaining%20is%20a%20foundational,answering%20and%20more) [[2]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=LangChain%20provides%20a%20powerful%20framework,LangChain%20handles%20prompt%20chaining%20effectively) [[10]](https://www.ibm.com/think/tutorials/prompt-chaining-langchain#:~:text=How%20LangChain%20manages%20prompt%20chaining) Prompt Chaining Langchain | IBM

[https://www.ibm.com/think/tutorials/prompt-chaining-langchain](https://www.ibm.com/think/tutorials/prompt-chaining-langchain)

[[3]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5) [[4]](https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow) [[5]](https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20works%20by%20processing%20a,real%20time%20to%20iteratively%20improve) [[6]](https://www.ibm.com/think/topics/autogpt#:~:text=In%20addition%20to%20GPT,return%20later%20to%20earlier%20projects) What is AutoGPT? | IBM

[https://www.ibm.com/think/topics/autogpt](https://www.ibm.com/think/topics/autogpt)

[[7]](https://arxiv.org/abs/2308.09687#:~:text=,advantages%20over%20state%20of%20the) [[8]](https://arxiv.org/abs/2308.09687#:~:text=paradigms%20such%20as%20Chain,We%20ensure%20that%20GoT%20is) [[9]](https://arxiv.org/abs/2308.09687#:~:text=approach%20enables%20combining%20arbitrary%20LLM,We%20ensure%20that%20GoT%20is) [[16]](https://arxiv.org/abs/2308.09687#:~:text=approach%20enables%20combining%20arbitrary%20LLM,both%20of%20which%20form%20complex) [2308.09687] Graph of Thoughts: Solving Elaborate Problems with Large Language Models

[https://arxiv.org/abs/2308.09687](https://arxiv.org/abs/2308.09687)

[[11]](https://blog.google/technology/ai/google-gemini-ai/#:~:text=match%20at%20L478%20We%27ve%20been,academic%20benchmarks%20used%20in%20large) Introducing Gemini: Google’s most capable AI model yet

[https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)

[[12]](https://arxiv.org/html/2407.06023v1#:~:text=the%20latter%20to%20expand%20on,final%20stage%20merges%20the%20results) Distilling System 2 into System 1

[https://arxiv.org/html/2407.06023v1](https://arxiv.org/html/2407.06023v1)

[[13]](https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a) [[14]](https://arxiv.org/abs/2312.10997#:~:text=synergistically%20merges%20LLMs%27%20intrinsic%20knowledge,faced%20and%20points%20out%20prospective) [[15]](https://arxiv.org/abs/2312.10997#:~:text=tripartite%20foundation%20of%20RAG%20frameworks%2C,avenues%20for%20research%20and%20development) [2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey

[https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)