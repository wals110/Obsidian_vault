
## Synthèse Exécutive

Ce rapport fournit une analyse technique approfondie de l'ingénierie des prompts pour les Grands Modèles de Langage (LLM), destinée aux ingénieurs en Machine Learning (ML) cherchant une maîtrise technique avancée. L'efficacité avec laquelle les LLM exécutent les tâches demandées est intrinsèquement liée à la qualité et à la structure des prompts qui leur sont soumis. Ce document explore les multiples facettes de cette discipline, depuis les mécanismes fondamentaux jusqu'aux applications les plus pointues et les perspectives d'avenir.

Les points clés abordés sont les suivants :
*   Les **fondements techniques**, incluant les mécanismes d'attention au sein des Transformers, l'impact crucial de la tokenisation, les distinctions architecturales entre les principaux modèles (GPT, Claude, Gemini, LLaMA) et la nature des représentations vectorielles, sont essentiels pour appréhender l'interaction complexe entre les prompts et le comportement des modèles.
*   Les **techniques avancées de prompting**, telles que le Chain-of-Thought (CoT) et ses variantes, l'In-Context Learning (ICL) avec ses mécanismes et limitations, le Few-Shot Learning, le chaînage de prompts et la Constitutional AI, démontrent comment des stratégies sophistiquées peuvent améliorer significativement le raisonnement, la précision et l'alignement des LLM sur des tâches complexes.
*   L'**optimisation algorithmique des prompts**, qu'elle soit basée sur des approches inspirées du gradient ou des méthodes évolutionnaires, ouvre la voie à l'automatisation de la découverte de prompts hautement performants, réduisant ainsi la dépendance à l'intuition humaine.
*   Les **spécificités par modèle**, notamment la gestion des contextes longs (par exemple, via Retrieval Augmented Generation - RAG), le prompting multimodal intégrant vision et texte, et l'intégration de "function calling" pour l'utilisation d'outils externes, étendent considérablement le champ des possibles pour les applications des LLM.
*   Les **applications spécialisées**, telles que la génération de code optimisée, le raisonnement scientifique dans des domaines comme les mathématiques et la logique, et la génération créative, illustrent l'impact transformateur de l'ingénierie des prompts dans des secteurs techniques et créatifs.
*   Un **guide pratique détaillé** aborde la structuration des prompts, les techniques de formulation efficaces, les stratégies de débogage et d'itération, ainsi que les anti-patterns à éviter, fournissant un cadre méthodologique pour le développement de prompts robustes.
*   L'**état de l'art pour la période 2022-2024** met en lumière une recherche extrêmement dynamique, caractérisée par des innovations constantes de la part d'acteurs académiques et industriels majeurs, tout en soulignant les limitations actuelles et les directions futures prometteuses pour le domaine.

Ce rapport s'efforce de maintenir un équilibre rigoureux entre les fondements théoriques et les applications pratiques, chaque concept étant étayé par des exemples concrets, des analyses de compromis et des méthodologies reproductibles, afin de permettre au lecteur de développer une expertise de pointe en ingénierie des prompts.

## 1. FONDEMENTS TECHNIQUES

La maîtrise de l'ingénierie des prompts repose sur une compréhension approfondie des mécanismes internes des Grands Modèles de Langage. Cette section explore les piliers techniques qui régissent la manière dont les LLM interprètent les instructions et génèrent des réponses, en mettant l'accent sur leur pertinence directe pour l'optimisation des prompts.

### 1.1 Mécanismes d'attention dans les transformers et optimisation des prompts

Le mécanisme d'attention est au cœur de l'architecture Transformer, qui sous-tend la majorité des LLM actuels. Sa fonction principale est de permettre au modèle de pondérer dynamiquement l'importance des différents segments de l'information d'entrée lors de la génération de chaque partie de la sortie.[1, 2]

**Fonctionnement du mécanisme d'attention :**
Initialement, le texte d'entrée (le prompt et tout contexte fourni) est converti en une séquence de représentations numériques appelées tokens. Chaque token est ensuite transformé en un vecteur via une consultation dans une table d'enchâssement lexical (embedding table). À chaque couche du Transformer, chaque token est contextualisé par rapport aux autres tokens (non masqués) présents dans la fenêtre de contexte. Cette contextualisation s'opère grâce à un mécanisme d'attention multi-têtes (multi-head attention) fonctionnant en parallèle. Ce processus a pour effet d'amplifier le signal des tokens jugés importants pour la tâche en cours et de diminuer l'influence des tokens moins pertinents.[1]

L'unité d'attention la plus courante dans les Transformers est l'attention par produit scalaire pondéré (scaled dot-product attention). Pour chaque unité, le modèle apprend trois matrices de poids distinctes :
1.  Les poids des Requêtes (Query weights, $W_Q$)
2.  Les poids des Clés (Key weights, $W_K$)
3.  Les poids des Valeurs (Value weights, $W_V$)

À partir d'un ensemble d'embeddings d'entrée, ces matrices sont utilisées pour dériver les vecteurs Query (Q), Key (K), et Value (V) pour chaque token. L'attention est alors calculée selon la formule :
$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$
où $d_k$ est la dimension des vecteurs de clé. Le terme $\sqrt{d_k}$ est un facteur de mise à l'échelle crucial qui stabilise les gradients durant l'entraînement, évitant que les produits scalaires ne deviennent trop grands.[1] La fonction softmax normalise ensuite les scores d'attention, les transformant en une distribution de probabilités qui indique le poids à accorder à chaque token de valeur.

Un aspect important est la non-symétrie de l'attention : le fait que $Q$ et $K$ soient issues de transformations linéaires distinctes permet à l'attention d'être non symétrique. Si un token $i$ porte une attention élevée à un token $j$ (c'est-à-dire que le produit scalaire $q_i \cdot k_j$ est grand), cela n'implique pas nécessairement que le token $j$ portera une attention élevée au token $i$.[1]

**Pertinence pour l'optimisation des prompts :**
La compréhension fine de ce mécanisme est fondamentale pour l'ingénierie des prompts. La formulation d'un prompt – le choix des mots, leur ordre, la structure globale – influence directement la manière dont les vecteurs Q, K, et V sont générés et interagissent. Par conséquent, un prompt bien conçu est celui qui guide efficacement le mécanisme d'attention du modèle vers les éléments d'information les plus critiques pour la tâche demandée. Par exemple, placer des instructions clés au début du prompt ou utiliser une formulation qui maximise la "saillance" des termes importants peut aider à focaliser l'attention du modèle. Inversement, des prompts ambigus ou mal structurés peuvent diluer l'attention ou l'orienter vers des aspects non pertinents, conduisant à des réponses de moindre qualité. L'optimisation des prompts cherche donc, en partie, à "sculpter" le paysage attentionnel que le modèle va explorer.

Le tableau suivant résume les mécanismes d'attention clés :

| Type d'Attention | Description du Mécanisme | Rôle des Matrices Q, K, V | Formule Mathématique (simplifiée) | Avantages | Implications pour le Prompting |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Scaled Dot-Product | Calcule les scores d'attention en mesurant la similarité entre une requête (Query) et un ensemble de clés (Keys), puis utilise ces scores pour pondérer les valeurs (Values). | Q représente la requête actuelle, K représente les clés des tokens auxquels on compare, V représente l'information à extraire de ces tokens. | $softmax(\frac{QK^T}{\sqrt{d_k}})V$ | Efficacité computationnelle, stabilité des gradients grâce à la mise à l'échelle. | La formulation du prompt doit viser à créer des Queries qui s'alignent fortement avec les Keys des informations pertinentes. |
| Multi-Head Attention | Exécute le mécanisme d'attention scaled dot-product plusieurs fois en parallèle avec différentes projections linéaires apprises de Q, K, et V. Les sorties sont concaténées et à nouveau projetées. | Permet au modèle d'apprendre conjointement des informations provenant de différents sous-espaces de représentation et à différentes positions. Chaque "tête" peut se concentrer sur des aspects différents. | $\text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O \text{ où } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ | Capacité à capturer différents types de relations et de dépendances dans les données. Meilleure performance. | Un prompt peut nécessiter d'activer différents "types" d'attention. La richesse sémantique et structurelle du prompt peut aider le modèle à utiliser ses multiples têtes de manière plus efficace pour désambiguïser ou lier des concepts distants. |

L'interaction entre la tokenisation, l'attention et l'architecture globale du modèle est un point déterminant. La manière dont un prompt est initialement découpé en tokens (section 1.2) conditionne les unités sur lesquelles l'attention va opérer. Ces tokens vectorisés sont ensuite pondérés par l'attention, et c'est l'architecture spécifique du LLM (section 1.3), avec ses couches d'attention successives et ses autres composants, qui va agréger et transformer ces signaux attentionnels pour finalement produire une représentation interne (section 1.4) et une sortie. Une modification apparemment mineure dans la formulation d'un prompt peut ainsi entraîner une cascade d'effets à travers ces étapes, soulignant la sensibilité des LLM aux prompts.

### 1.2 Impact de la tokenisation sur l'efficacité des prompts

La tokenisation est l'étape préliminaire indispensable par laquelle le texte brut d'un prompt est converti en une séquence d'unités discrètes, les tokens, que le LLM peut effectivement traiter.[3] Ces tokens peuvent correspondre à des mots, des sous-mots (subwords), des caractères, ou même des symboles, en fonction de l'algorithme de tokenisation employé (par exemple, Byte Pair Encoding (BPE), WordPiece, SentencePiece).[3] Ce processus est fondamental car les LLM opèrent sur des représentations numériques de ces tokens, et non directement sur le texte.[3, 4]

**Impacts directs de la tokenisation sur l'efficacité des prompts :**

*   **Incohérence d'assignation d'ID et sensibilité à la casse :** Les tokeniseurs peuvent assigner des identifiants (ID) de tokens différents à des mots qui sont sémantiquement similaires mais qui diffèrent par la casse (ex: "Prompt" vs "prompt") ou par de légères variations morphologiques. Par exemple, "John" pourrait être un token unique, tandis que "John Rickard" pourrait être scindé en "John", "Rick", et "ard", chacun avec son propre ID.[3] Cette fragmentation ou différenciation peut amener le modèle à interpréter des concepts identiques comme étant distincts, ce qui affecte la cohérence de sa compréhension et peut introduire des comportements inattendus en réponse à des prompts subtilement variés.[3]
*   **Gestion des espaces et des chiffres :** La manière dont les espaces (en particulier les espaces en fin de mot ou de phrase) et les séquences de chiffres sont tokenisés peut varier considérablement. Un espace peut être un token à part entière ou fusionné avec un mot. De même, un nombre comme "123" peut être un token unique, tandis que "12o" (si "o" est considéré comme faisant partie d'une unité) pourrait être un token et "124" pourrait être "12" et "4".[3] Ces variations influencent la segmentation du prompt et, par conséquent, la prédiction des tokens suivants par le modèle.[3]
*   **Tokenisation spécifique au modèle :** Chaque famille de LLM (et parfois chaque version spécifique d'un modèle) utilise son propre tokeniseur, entraîné sur un vocabulaire particulier. Un prompt qui est découpé d'une certaine manière par le tokeniseur de GPT-4 peut être découpé différemment par celui de Claude ou de LLaMA.[3] Cela signifie qu'un prompt optimisé pour un modèle n'est pas nécessairement optimal pour un autre, même si l'intention sémantique est identique. L'utilisation du tokeniseur spécifique au modèle est donc une pratique essentielle.[3]
*   **Taille du vocabulaire et gestion des mots hors vocabulaire (OOV) :** Les tokeniseurs basés sur les sous-mots, comme BPE, sont conçus pour gérer les mots rares ou inconnus (OOV) en les décomposant en unités plus petites et plus fréquentes présentes dans le vocabulaire.[3] Par exemple, "unhappiness" peut devenir ["un", "happi", "ness"].[3] Si cela améliore la robustesse du modèle face à un vocabulaire diversifié, une décomposition excessive de mots importants peut parfois diluer leur signification ou augmenter la longueur de la séquence de tokens.
*   **Efficacité computationnelle et limites de contexte :** La tokenisation a un impact direct sur le nombre de tokens qu'un prompt génère. Étant donné que les LLM ont des fenêtres de contexte de taille limitée (le nombre maximum de tokens qu'ils peuvent traiter en une fois) et que le coût d'utilisation des API est souvent lié au nombre de tokens, une tokenisation efficace est cruciale.[4] Des choix de formulation dans un prompt peuvent conduire à un nombre de tokens significativement différent pour une même intention sémantique. Par exemple, utiliser des synonymes plus courts ou éviter des formulations inutilement verbeuses peut aider à rester dans les limites du contexte et à réduire les coûts.

**Bonnes pratiques et considérations pour l'ingénieur de prompts :**
Il est impératif de comprendre le comportement du tokeniseur du LLM cible. Des outils permettent souvent de visualiser comment un texte est tokenisé. Selon une étude, pour les modèles multilingues, l'utilisation de tokens uniquement anglais n'est pas optimale et peut dégrader les performances et augmenter le temps de réponse.[4] L'algorithme BPE est souvent un bon choix pour les modèles mono et multilingues, et une taille de vocabulaire d'environ 33 000 tokens est considérée comme optimale pour les modèles centrés sur l'anglais.[4]

La "localité" de l'attention (abordée en 1.1) est également affectée par la tokenisation. Une tokenisation qui produit un grand nombre de tokens pour une phrase conceptuellement courte augmente la "distance" que les mécanismes d'attention doivent parcourir pour relier des idées. Pour les prompts longs, cela peut exacerber le problème de la "perte au milieu", où les informations situées au centre d'un long contexte sont moins bien prises en compte. Une formulation concise et une tokenisation efficace peuvent donc aider à maintenir la pertinence des informations sur toute la longueur du prompt.

Le tableau suivant illustre l'impact de la tokenisation :

| Problème de Tokenisation | Description | Exemple Illustratif (Conceptuel) | Impact sur le Prompt | Recommandation pour l'Ingénieur de Prompts |
| :--- | :--- | :--- | :--- | :--- |
| Incohérence d'assignation d'ID / Casse | Des mots sémantiquement proches ou identiques mais avec des variations de casse/forme sont tokenisés en unités/ID distincts.[3] | "IA" vs "Intelligence Artificielle" ; "Chatbot" vs "chatbot". "John" ->  ; "John Rickard" ->.[3] | Peut entraîner une mauvaise interprétation de l'intention ou une incapacité à reconnaître des concepts équivalents. | Utiliser une casse cohérente. Tester les variations pour comprendre comment le modèle les traite. Être conscient que des noms propres peuvent être fragmentés. |
| Gestion des espaces et des chiffres | Les espaces (surtout en fin de mot) et les chiffres peuvent être tokenisés de manière inattendue, affectant la prédiction suivante.[3] | "  exemple" vs "exemple" ; "12o" -> [token_A] vs "124" ->.[3] | Peut modifier la probabilité du token suivant ou la compréhension des valeurs numériques. | Normaliser les espaces. Faire attention à la manière dont les nombres sont formatés, surtout si des opérations mathématiques sont attendues. |
| Variabilité inter-modèles | Différents LLM utilisent différents tokeniseurs (BPE, WordPiece, SentencePiece) et vocabulaires.[3] | Un prompt P1 tokenisé en N tokens pour le modèle M1 peut être tokenisé en M tokens (M ≠ N) pour le modèle M2. | Un prompt optimisé pour un modèle peut être sous-optimal pour un autre. | Toujours utiliser le tokeniseur spécifique au LLM cible pour le développement et l'optimisation des prompts. Vérifier le nombre de tokens générés. |
| Mots Hors Vocabulaire (OOV) / Sous-mots | Les mots inconnus sont décomposés en sous-mots connus, ce qui peut parfois altérer la sémantique fine.[3] | "Supercalifragilisticexpialidocious" ->. | Peut rendre certains termes techniques ou néologismes moins impactants ou plus difficiles à interpréter pour le modèle si trop fragmentés. | Pour les termes techniques cruciaux, vérifier leur tokenisation. Si un terme est systématiquement mal décomposé, envisager des reformulations ou l'utilisation d'acronymes définis dans le prompt si le modèle peut les gérer. |
| Efficacité / Coût | Le nombre de tokens affecte la vitesse de traitement, le coût de l'API, et la capacité à tenir dans la fenêtre de contexte.[4] | Un prompt verbeux peut dépasser la limite de tokens ou coûter plus cher qu'un prompt concis équivalent. | Des prompts inutilement longs peuvent être tronqués ou entraîner des coûts plus élevés. | Privilégier la concision sans sacrifier la clarté. Utiliser des abréviations ou des formulations compactes lorsque le sens est préservé. Évaluer régulièrement la longueur en tokens des prompts types. |

### 1.3 Différences architecturales : GPT vs Claude vs Gemini vs LLaMA (et leurs implications pour le prompting)

Bien que la majorité des LLM modernes s'appuient sur l'architecture Transformer, des variations significatives dans leur conception, leur entraînement et leurs capacités spécifiques ont un impact direct sur les stratégies de prompting les plus efficaces. Comprendre ces nuances est essentiel pour l'ingénieur ML souhaitant maximiser la performance de ses prompts sur différents modèles.

*   **Série GPT (OpenAI) :**
    *   **Caractéristiques :** Les modèles GPT, tels que GPT-4, GPT-4 Turbo, et le plus récent GPT-4o, sont réputés pour leur polyvalence et leurs capacités de compréhension et de génération de texte avancées.[5] GPT-4o se distingue par ses capacités "omni-modales" (texte, image, audio) et une meilleure gestion de la mémoire pour les conversations étendues.[5] Une variante comme O3 Mini est optimisée pour les domaines scientifiques et techniques (STEM), y compris le codage.[5]
    *   **Implications pour le prompting :** La grande flexibilité de GPT permet des prompts créatifs et complexes. Cependant, ces modèles peuvent parfois "halluciner" (générer des informations fausses mais plausibles) ou ignorer des cas limites, ce qui nécessite des prompts qui encouragent la vérification ou qui fournissent des contraintes claires.[6] La multimodalité de GPT-4o ouvre la porte à des prompts qui combinent et font interagir différents types de données (par exemple, demander une description textuelle d'une image fournie).

*   **Série Claude (Anthropic) :**
    *   **Caractéristiques :** Les modèles Claude sont conçus pour des conversations plus réfléchies, nuancées et souvent plus sûres. Claude 3 Opus excelle dans le traitement de textes complexes (juridiques, académiques), tandis que Claude 3.5 Sonnet montre une forte créativité et Claude 3 Haiku est optimisé pour la concision.[5] Une caractéristique notable est leur grande fenêtre de contexte (par exemple, 200 000 tokens pour Claude 3.5 Sonnet).[6] L'entraînement de Claude intègre la "Constitutional AI", une approche d'alignement basée sur des principes explicites.
    *   **Implications pour le prompting :** La fenêtre de contexte étendue est un atout majeur, permettant des prompts qui incluent des documents volumineux, des historiques de conversation détaillés, ou des ensembles d'instructions très riches.[6] Les prompts peuvent donc être plus longs et plus exhaustifs. L'alignement via Constitutional AI peut rendre Claude particulièrement attentif aux aspects éthiques et de sécurité, ce qui peut influencer sa manière de répondre à des prompts sensibles ou ambigus.[6] Des prompts bien structurés sont souvent bénéfiques pour obtenir des sorties précises.

*   **Série Gemini (Google) :**
    *   **Caractéristiques :** Les modèles Gemini, comme Gemini 1.5 Pro, sont souvent intégrés à l'écosystème Google (Workspace, Cloud AI) et peuvent bénéficier d'un accès à des informations web en temps réel pour certaines applications.[6] Gemini 1.5 Pro est reconnu pour sa capacité à gérer le raisonnement structuré et le contenu technique.[5]
    *   **Implications pour le prompting :** Les prompts peuvent être conçus pour tirer parti de l'accès potentiel à des informations à jour, particulièrement utile pour des questions d'actualité ou des tâches nécessitant des données fraîches. L'intégration avec Google Cloud peut favoriser des prompts orientés vers des workflows d'entreprise spécifiques à cet environnement.

*   **Série LLaMA (Meta AI) :**
    *   **Caractéristiques :** LLaMA et ses successeurs (comme LLaMA 3) sont des modèles dont les poids sont souvent ouverts, ce qui permet une personnalisation et un fine-tuning poussés par la communauté de recherche et les entreprises.[6] Des versions comme LLaMA 3-70B sont performantes pour la génération de code et les tâches logiques.[5] Ils peuvent être déployés localement, offrant des avantages en termes de confidentialité et de contrôle.
    *   **Implications pour le prompting :** L'ouverture des poids signifie que le "prompting" peut s'étendre au-delà de la simple formulation de requêtes à l'inférence. Il peut inclure l'adaptation du modèle lui-même par fine-tuning sur des données et des styles de prompts spécifiques. Les prompts pour les modèles LLaMA de base ("out-of-the-box") peuvent nécessiter plus d'explicitation et d'exemples que pour les modèles commerciaux hautement instruits et alignés, car leur performance brute peut être inférieure sans cette adaptation.[6] La possibilité de déploiement local est un facteur clé pour les applications manipulant des données sensibles.

Le tableau suivant synthétise ces différences architecturales et leurs conséquences pour le prompting :

| Modèle (Exemple) | Architecture de Base (Supposée) | Caractéristiques Distinctives | Forces pour le Prompting | Faiblesses/Considérations pour le Prompting |Stratégies de Prompting Recommandées |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **GPT-4o (OpenAI)** | Transformer | Omni-modal (texte, image, audio), grande capacité de compréhension, bonne mémoire contextuelle.[5] | Très polyvalent, créativité, tâches complexes, multimodalité. | Peut halluciner, peut nécessiter une vérification des faits.[6] | Prompts clairs et spécifiques, utilisation d'exemples (few-shot), décomposition de tâches, prompts multimodaux. |
| **Claude 3.5 Sonnet (Anthropic)** | Transformer | Très grande fenêtre de contexte (200K tokens) [6], Constitutional AI, réponses réfléchies. | Traitement de documents longs, conversations multi-tours, tâches nécessitant nuance et sécurité. | Peut être légèrement plus lent [6], peut nécessiter des prompts plus structurés pour des formats de sortie spécifiques. | Exploiter la grande fenêtre de contexte, prompts détaillés, prise en compte des principes éthiques, structuration pour sorties spécifiques. |
| **Gemini 1.5 Pro (Google)** | Transformer | Intégration écosystème Google, potentiel accès web temps réel [6], bon en raisonnement structuré.[5] | Tâches nécessitant des informations à jour, workflows Google Cloud, contenu technique. | Traction communautaire pour développeurs encore en croissance.[6] | Prompts exploitant l'accès aux données externes, prompts pour raisonnement structuré. |
| **LLaMA 3 (Meta AI)** | Transformer | Poids ouverts, personnalisable, déployable localement.[6] | Fine-tuning pour tâches spécifiques, contrôle des données, applications respectueuses de la vie privée. | Performance out-of-the-box peut être inférieure aux modèles commerciaux sans fine-tuning [6], nécessite une expertise pour l'adaptation. | Combinaison de prompting et de fine-tuning, prompts plus explicites pour les modèles de base, exploitation de la flexibilité open-source. |

Ces différences architecturales et de philosophie d'entraînement impliquent qu'une stratégie de prompt "universelle" est illusoire. L'ingénieur doit adapter son approche en fonction du modèle cible, en exploitant ses forces et en mitigeant ses faiblesses.

### 1.4 Représentations vectorielles et aspects géométriques des formulations de prompts

Lorsqu'un prompt est soumis à un LLM, il subit une série de transformations. Après la tokenisation (section 1.2), les tokens sont convertis en représentations vectorielles denses, appelées embeddings, qui existent dans un espace de très haute dimension. La "géométrie" de ces embeddings – c'est-à-dire leur distribution spatiale, leurs relations de proximité, et la manière dont ils forment des structures ou des "manifolds" correspondant à des concepts ou des catégories – joue un rôle crucial dans la façon dont le modèle interprète le prompt et génère une réponse.[7, 8]

**Influence de la formulation du prompt sur les représentations latentes :**
Des recherches ont montré que la formulation spécifique d'un prompt a une influence mesurable sur les représentations latentes (les activations internes du modèle à différentes couches).[7] Il est important de noter que cet impact peut varier en fonction de la famille de modèles LLM considérée. Une observation clé est que des prompts qui peuvent sembler sémantiquement similaires en langage naturel peuvent en réalité aboutir à des représentations vectorielles distinctes dans l'espace latent du modèle.[7] Cette sensibilité à la formulation explique en partie pourquoi de légères modifications d'un prompt peuvent parfois entraîner des changements significatifs dans la sortie du LLM. L'étude de Kirsanov et al. (2025) [8, 9, 10] souligne que différentes techniques de prompting, même si elles atteignent des performances similaires, opèrent à travers des mécanismes de représentation distincts pour l'adaptation à la tâche.

**Manifolds de catégories et capacité de manifold :**
Dans le contexte des tâches de classification, les exemples (ou les prompts) qui appartiennent à une même catégorie (par exemple, "sentiment positif" ou "question technique") tendent à former des regroupements ou des "nuages de points" dans l'espace d'embedding. Ces structures sont appelées des "manifolds de catégories".[8] La "capacité de manifold" est une mesure issue de la physique statistique qui quantifie l'efficacité avec laquelle les caractéristiques pertinentes pour une tâche sont encodées, du point de vue d'un décodeur linéaire en aval. Essentiellement, elle mesure la séparabilité des manifolds des différentes classes cibles dans cet espace d'embedding.[8] Une bonne séparabilité indique un encodage efficace. Différentes approches de prompting (comme le fine-tuning complet, les prompts statiques en zero-shot ou few-shot, ou le prompt-tuning où des embeddings de "soft prompts" sont appris) affectent différemment ces géométries et, par conséquent, la capacité de manifold pour une tâche donnée.[8, 9, 10]

**Recherche vectorielle et son lien avec les prompts :**
La recherche vectorielle est une technique de plus en plus utilisée en conjonction avec les LLM, notamment dans les systèmes de Retrieval Augmented Generation (RAG). Dans ce paradigme, de vastes corpus de documents sont d'abord encodés sous forme de vecteurs (embeddings) et stockés dans une base de données vectorielles. Lorsqu'une requête utilisateur est formulée, elle est également transformée en vecteur, et une recherche de similarité (cosinus, par exemple) est effectuée pour trouver les "chunks" (morceaux) de documents les plus pertinents. Ces chunks récupérés sont ensuite injectés dans le prompt final soumis au LLM, lui fournissant un contexte ciblé.[11] La qualité du "chunking" (la manière de diviser les documents en morceaux cohérents et de taille gérable) et la qualité des embeddings de ces chunks sont déterminantes pour la pertinence des informations fournies au LLM et donc pour la qualité de la réponse finale.[11]

**Limitations et complexité de l'analyse géométrique :**
L'analyse de la géométrie des représentations latentes est un domaine de recherche actif mais complexe. Certaines études, comme celle référencée dans [7], peuvent se concentrer sur des aspects simplifiés, tels que la représentation du token de fin de séquence (EOS) ou des tâches de classification binaire, ce qui peut ne pas capturer toute la complexité des tâches de génération ouverte.[7] De plus, établir une corrélation directe et univoque entre des mesures géométriques spécifiques (comme l'IsoScore mentionné dans [7] mais non défini dans les extraits disponibles) et la performance globale du modèle sur une tâche n'est pas toujours simple.[7] Le lien entre les prompts, les représentations latentes et la performance du modèle est un objectif de recherche, mais les résultats peuvent être nuancés et dépendants du modèle.[7] Des travaux plus récents [12, 13] continuent d'explorer comment les caractéristiques sémantiques et structurelles sont encodées dans ces manifolds.

La formulation d'un prompt peut donc être vue comme une tentative de "naviguer" dans cet espace latent complexe. L'objectif est de trouver une formulation qui projette l'intention de l'utilisateur dans une région de l'espace d'embedding qui est "proche" des connaissances et des capacités du modèle nécessaires pour accomplir la tâche souhaitée. Cette perspective offre un fondement théorique aux techniques d'optimisation algorithmique des prompts (abordées en Section 3), qui explorent systématiquement l'espace des formulations possibles pour identifier celles qui maximisent la performance. La sensibilité des LLM aux variations de prompts n'est donc pas simplement un artefact, mais une conséquence de la manière dont le langage et les concepts sont encodés et inter-reliés dans ces vastes espaces vectoriels. Une petite modification textuelle peut induire un déplacement significatif dans cet espace, menant à une interprétation et une réponse potentiellement très différentes de la part du modèle.

## 2. TECHNIQUES AVANCÉES DE PROMPTING

Au-delà des instructions directes, un éventail de techniques de prompting plus sophistiquées a émergé pour débloquer des capacités de raisonnement plus complexes, une meilleure adaptation contextuelle et un contrôle plus fin sur les sorties des Grands Modèles de Langage. Ces stratégies sont cruciales pour aborder des tâches qui dépassent la simple récupération d'informations ou la génération de texte basique.

### 2.1 Chain-of-Thought (CoT) : variantes récentes, efficacité par domaine

Le prompting Chain-of-Thought (CoT) est une technique qui incite les LLM à décomposer un problème complexe en une séquence d'étapes de raisonnement intermédiaires avant de parvenir à une réponse finale.[14, 15] Cette approche est généralement mise en œuvre en fournissant au modèle quelques exemples (démonstrations) dans le prompt, où chaque exemple illustre non seulement la question et la réponse, mais aussi le processus de pensée détaillé qui mène de l'une à l'autre.[14] L'objectif est d'amener le LLM à "réfléchir à voix haute", améliorant ainsi ses performances sur des tâches qui exigent un raisonnement logique, des calculs mathématiques ou une manipulation symbolique, domaines où les LLM peuvent traditionnellement éprouver des difficultés.[14]

Le mécanisme CoT agit comme un guide contextuel ; il n'altère pas les capacités intrinsèques du modèle mais sert plutôt d'augmentation externe de connaissances, activant les schémas de raisonnement pertinents pour la tâche en cours.[14]

**Variantes récentes et améliorations (2023-2024) :**
Le domaine du CoT est en constante évolution, avec plusieurs variantes notables :

*   **CoT basé sur les "Reasoning Patterns" (Motifs de Raisonnement) :** Les méthodes CoT non supervisées traditionnelles sélectionnent souvent des exemples de démonstration en se basant sur la similarité sémantique des questions, ce qui peut introduire du bruit et manquer d'interprétabilité.[14] Une approche plus récente propose de s'appuyer sur les "motifs de raisonnement" sous-jacents, c'est-à-dire la structure ou le template par lequel le modèle arrive à la solution (par exemple, la structure des équations, l'enchaînement des étapes logiques).[14, 16] En extrayant ces motifs (par exemple, à partir d'un ensemble initial de démonstrations générées en mode zero-shot ou issues d'un ensemble d'entraînement) et en les regroupant (par exemple, par clustering), on peut sélectionner un ensemble diversifié de démonstrations qui illustrent différents chemins de raisonnement.[16] Cette méthode vise à réduire le bruit, à améliorer la robustesse et à offrir une meilleure interprétabilité des mécanismes CoT.[14] Des études ont montré que l'exactitude des exemples individuels est souvent moins cruciale que la cohérence et la clarté du motif de raisonnement qu'ils présentent.[14]
*   **Zero-Shot CoT :** Une avancée significative est la capacité à induire un raisonnement de type CoT sans fournir d'exemples spécifiques. Une simple instruction ajoutée au prompt, telle que "Réfléchissons étape par étape" ("Let's think step by step"), peut suffire à ce que le modèle génère de lui-même une chaîne de pensée avant de donner la réponse finale.[17] Cette approche simplifie grandement l'application du CoT.
*   **Autres variantes notables (issues de "The Prompt Report" [18]) :**
    *   *Contrastive CoT :* Implique la génération de chaînes de pensée pour des options correctes et incorrectes afin d'améliorer la discrimination.
    *   *Uncertainty-Routed CoT :* Le modèle peut exprimer son incertitude et potentiellement demander des clarifications ou des étapes supplémentaires.
    *   *Complexity-based Prompting :* Adapte la complexité du CoT en fonction de la complexité perçue de la question.
    *   *Active Prompting :* Le modèle sélectionne les exemples les plus informatifs pour construire sa chaîne de pensée.
    *   *Memory-of-Thought (MoT) :* Permet au modèle de se référer à des étapes de pensée antérieures ou à des connaissances stockées.
    *   *Automatic Chain-of-Thought (Auto-CoT) :* Vise à automatiser la génération des démonstrations CoT elles-mêmes.
    *   *Tabular Chain-of-Thought (Tab-CoT) :* Spécialisé pour le raisonnement sur des données tabulaires.
    *   *Thread-of-Thought (ThoT) :* Explore plusieurs fils de pensée en parallèle.

**Efficacité par domaine :**
Le CoT et ses variantes se sont montrés particulièrement efficaces pour améliorer les performances des LLM dans les domaines nécessitant un raisonnement déductif, inductif ou multi-étapes. Cela inclut le raisonnement arithmétique, la résolution de problèmes mathématiques, le raisonnement de bon sens, le raisonnement symbolique et certaines tâches de planification.[14] L'ampleur de l'amélioration dépend de la complexité de la tâche, de la qualité des démonstrations (ou de l'instruction zero-shot) et des capacités intrinsèques du modèle LLM utilisé.

Le tableau suivant compare quelques variantes clés de CoT :

| Variante de CoT | Description du Mécanisme | Avantages | Inconvénients/Limitations | Domaines d'Application Privilégiés | Exemple de Structure de Prompt (Conceptuel) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Standard Few-Shot CoT** | Fournit 2-5 exemples complets (question, étapes de raisonnement, réponse) dans le prompt.[14, 15] | Guidage explicite, améliore le raisonnement sur tâches complexes. | Nécessite la création manuelle d'exemples de haute qualité, sensible au choix des exemples. | Raisonnement arithmétique, problèmes de logique, Q&A multi-étapes. | `Q: [Exemple 1 Q] \n A: \n... \n Q: [Question Actuelle] \n A:` |
| **Zero-Shot CoT** | Ajoute une simple phrase comme "Réfléchissons étape par étape" au prompt, sans exemples.[17] | Très simple à implémenter, ne nécessite pas d'exemples. Efficace pour les LLM très capables. | Peut être moins performant que le few-shot pour des tâches très complexes ou des modèles moins avancés. | Tâches de raisonnement générales où le modèle a déjà une forte capacité intrinsèque. | `Q: [Question Actuelle] \n A: Réfléchissons étape par étape.` |
| **Reasoning Patterns CoT** | Sélectionne des démonstrations basées sur la diversité des "motifs de raisonnement" plutôt que sur la sémantique de la question.[14, 16] | Réduit le bruit des exemples non pertinents, améliore la robustesse et l'interprétabilité. | Nécessite une étape d'extraction et de clustering des motifs de raisonnement. | Tâches où différents types de logique peuvent être appliqués (ex: problèmes mathématiques avec différentes approches de résolution). | ` \n Q: [Question Actuelle] \n A:` |
| **Auto-CoT** | Automatise la génération des démonstrations CoT, souvent en utilisant un LLM pour générer des questions et leurs chaînes de pensée.[18] | Réduit l'effort manuel de création d'exemples. Peut découvrir des CoT non intuitifs. | La qualité des CoT générés automatiquement peut varier. Nécessite une évaluation rigoureuse. | Scénarios où la création manuelle de CoT est prohibitive (grand nombre de tâches, domaines nouveaux). | (Processus automatisé, le prompt final ressemblerait à un Few-Shot CoT mais avec des exemples générés). |

L'une des observations importantes qui se dégage de l'étude des techniques CoT est la possibilité d'une tension entre l'explicitation du raisonnement et la performance brute. Bien que le CoT vise à améliorer le raisonnement en forçant la génération d'étapes intermédiaires, certaines études, notamment sur des tâches de raisonnement scientifique avec des modèles comme GPT-4o [17, 19], ont montré que des approches plus directes (comme le "Direct Answer" en zero-shot) peuvent parfois atteindre une précision comparable, voire supérieure, au CoT few-shot traditionnel. La technique de "Self-Consistency" (qui génère plusieurs CoT et agrège les réponses) tend à offrir la meilleure précision, mais peut être moins transparente dans son explication. Cela suggère que les mécanismes internes par lesquels un LLM parvient à une solution peuvent être plus complexes ou différer d'un processus de pensée humain linéaire. L'ingénieur de prompts doit donc considérer si l'objectif principal est la réponse la plus exacte possible ou la démonstration la plus claire du cheminement vers cette réponse.

### 2.2 In-context learning (ICL) : mécanismes, limitations, optimisations 2023-2024

L'In-Context Learning (ICL) est une capacité remarquable des Grands Modèles de Langage qui leur permet d'"apprendre" ou de s'adapter à une nouvelle tâche en se basant uniquement sur des exemples (démonstrations) fournis directement dans le prompt, sans nécessiter de mise à jour des poids du modèle.[20, 21] Le Chain-of-Thought (CoT) est une forme spécifique d'ICL, axée sur la décomposition du raisonnement.

**Mécanismes sous-jacents (Théories) :**
La compréhension des mécanismes exacts de l'ICL est un domaine de recherche actif. Plusieurs hypothèses ont été proposées :

*   **Apprentissage implicite basé sur le gradient / Optimisation :** Une ligne de recherche suggère que les Transformers, lorsqu'ils traitent des exemples en contexte, effectuent une forme d'optimisation implicite. Cela pourrait s'apparenter à des mises à jour de type descente de gradient sur une fonction de perte interne, à une optimisation d'ordre supérieur, ou à une forme d'inférence bayésienne approximative.[20, 21] Par exemple, Akyürek et al. (2022) ont démontré que, par construction, les Transformers peuvent implémenter des algorithmes d'apprentissage pour des modèles linéaires, en émulant la descente de gradient ou la régression ridge en forme close.[21] Dai et al. (2023) voient également l'attention dans les Transformers comme ayant une forme duale de la descente de gradient, positionnant les Transformers comme des méta-optimiseurs.[21]
*   **Mémorisation et reconnaissance de patterns via les matrices d'attention :** D'autres travaux mettent en avant le rôle crucial de la mémorisation. Les modèles s'appuieraient fortement sur les matrices d'attention pour identifier et reproduire des patterns vus dans les exemples fournis.[20] Olsson et al. (2022) et Naim & Asher (2024) soutiennent que l'attention est le mécanisme principal permettant l'ICL.[20]

**Limitations fondamentales identifiées (2023-2024) :**
Malgré ses capacités impressionnantes, l'ICL présente des limitations inhérentes :

*   **"Boundary Values" (Valeurs Limites) :** Des études menées sur des tâches d'approximation de fonctions (par exemple, des fonctions linéaires ou polynomiales) avec des Transformers entraînés à partir de zéro ont révélé un phénomène de "valeurs limites".[20, 22] Les modèles peuvent effectuer de l'ICL et généraliser correctement dans une certaine plage de valeurs d'entrée, mais leurs performances se dégradent brusquement (passant de prédictions précises à des sorties constantes ou quasi-aléatoires) lorsque les entrées dépassent un certain seuil, s'éloignant de la distribution vue pendant l'entraînement.[20]
*   **Rôle de la Layer Normalization (LN) et de la fonction Softmax :**
    *   **Layer Normalization :** Des études d'ablation ont identifié la LN comme le principal contributeur à l'existence de ces valeurs limites.[20, 22] En normalisant les activations à chaque couche, la LN contraint leur magnitude, empêchant le modèle d'extrapoler de manière significative au-delà de la plage de valeurs rencontrée durant son pré-entraînement. La suppression de la LN dans des expériences contrôlées a permis aux modèles de produire des valeurs de sortie beaucoup plus grandes, éliminant de fait ces "boundary values".[20] Mathématiquement, il a été montré (Proposition 3 dans Naim & Asher, 2024 [22]) que la sortie de la LN tend asymptotiquement vers une constante lorsque la norme de l'entrée tend vers l'infini, expliquant l'apparition de ces seuils.
    *   **Softmax :** La fonction softmax, utilisée pour calculer les poids d'attention, présente également des comportements qui peuvent limiter l'ICL.[22] Lorsque les valeurs d'entrée (scores d'attention bruts) sont très disparates, softmax tend à se comporter comme une fonction "hardmax", attribuant un poids proche de 1 à la valeur la plus élevée et des poids proches de 0 aux autres. Cela peut amener le modèle à se concentrer sur un seul token ou exemple, ignorant le reste du contexte. Inversement, pour de longues séquences avec des valeurs d'entrée similaires, softmax peut tendre vers une distribution uniforme, diluant l'attention sur tous les tokens et rendant difficile la focalisation sur les exemples les plus pertinents.[22]
*   **Dépendance à la structure du prompt et à la taille du modèle :** L'efficacité de l'ICL est fortement sensible à la manière dont les prompts sont structurés, à l'ordre dans lequel les exemples sont présentés, et à la taille globale du modèle.[21] Il est intéressant de noter que l'exactitude des étiquettes dans les démonstrations n'est pas toujours le facteur le plus critique pour un ICL efficace ; la distribution des entrées et le format des exemples peuvent avoir un impact plus important.[21]

**Optimisations et évolutions (2023-2024) :**

*   **Exploitation des contextes longs :** Avec l'avènement de modèles capables de traiter des contextes de plus en plus longs (allant jusqu'à 10 millions de tokens pour certains modèles expérimentaux [21]), les performances de l'ICL peuvent être significativement améliorées en fournissant un plus grand nombre de démonstrations. Dans certains cas, avec suffisamment d'exemples, l'ICL peut atteindre des performances comparables à celles obtenues par fine-tuning sur la même quantité de données.[21]
*   **Knowledge-Guided Prompting (KGP) :** Cette approche, proposée par Seedat et al. (2024) [23], vise à réduire la dépendance aux exemples ICL, voire à les éliminer complètement. L'idée est d'infuser explicitement des connaissances du domaine (qu'elles soient symboliques, statistiques, ou autres a priori) directement dans le prompt. Ces connaissances guidant le modèle, elles servent de "levier" supplémentaire pour l'optimisation du prompt, diminuant le besoin d'illustrer la tâche par de multiples exemples.[23]

Ces limitations architecturales fondamentales de l'ICL, notamment les "boundary values" induites par la Layer Normalization, ont des implications directes pour l'ingénierie des prompts. Elles signifient que fournir des exemples qui sont quantitativement ou qualitativement trop éloignés de ce que le modèle a "l'habitude" de traiter (même implicitement via les contraintes de la LN) peut conduire à un échec de la généralisation. Cela est particulièrement pertinent pour les domaines scientifiques ou techniques où les grandeurs numériques peuvent varier sur de larges échelles. L'ingénieur de prompts doit donc être conscient de ces contraintes, potentiellement en normalisant les entrées ou en s'assurant que les exemples et la requête cible se situent dans une plage de valeurs "confortable" pour le modèle.

Le tableau suivant résume les mécanismes et limitations clés de l'ICL :

| Aspect de l'ICL | Description Détaillée | Évidence Empirique/Théorique | Implication pour l'Ingénieur de Prompts |
| :--- | :--- | :--- | :--- |
| **Mécanisme : Gradient Implicite / Méta-Optimisation** | Les Transformers pourraient simuler des étapes d'optimisation (type descente de gradient) en traitant les exemples du prompt.[20, 21] | Travaux de Akyürek et al. (2022), Von Oswald et al. (2023), Dai et al. (2023).[21] | La structure et la séquence des exemples peuvent influencer ce "processus d'apprentissage implicite". L'ordre et la clarté des exemples sont donc primordiaux. |
| **Mécanisme : Mémorisation Attentionnelle** | L'ICL reposerait principalement sur la capacité de l'attention à retrouver et réutiliser des patterns présents dans les exemples fournis.[20] | Olsson et al. (2022), Naim & Asher (2024).[20] | Les exemples doivent être saillants et clairement délimités pour que l'attention puisse s'y "accrocher" efficacement. |
| **Limitation : Boundary Values** | Les modèles échouent à généraliser pour des valeurs d'entrée (ou des sorties attendues) trop éloignées de leur distribution d'entraînement ou des plages contraintes par l'architecture.[20, 22] | Observations empiriques sur tâches d'approximation de fonctions (Naim & Asher, 2024).[20] | Éviter de fournir des exemples ou des requêtes avec des valeurs numériques extrêmes par rapport à ce que le modèle peut gérer. Envisager la normalisation des données d'entrée si possible. |
| **Limitation : Rôle de Layer Normalization** | La LN contraint la magnitude des activations, créant des seuils prédictifs et limitant l'extrapolation.[20, 22] | Études d'ablation montrant l'élimination des boundary values sans LN.[20] Analyse mathématique (Proposition 3, Naim & Asher, 2024).[22] | Comprendre que la capacité d'extrapolation du modèle est structurellement limitée. Les prompts ne peuvent pas magiquement outrepasser ces contraintes. |
| **Limitation : Rôle de Softmax** | Le comportement de Softmax (hardmax pour entrées disparates, uniforme pour longues séquences similaires) peut nuire à l'utilisation sélective des exemples.[22] | Analyse du comportement de la fonction Softmax dans l'attention.[22] | Pour les prompts avec de nombreux exemples, s'assurer qu'ils sont suffisamment distincts. Pour les tâches nécessitant une attention fine sur des détails spécifiques, éviter de "noyer" ces détails dans un trop grand nombre d'exemples homogènes. |

### 2.3 Few-shot learning : sélection d'exemples, ordering effects

Le Few-Shot Learning est une application spécifique de l'In-Context Learning (ICL) où un petit nombre d'exemples de démonstration (typiquement de 1 à 5, appelés "shots") sont inclus directement dans le prompt.[15] Chaque exemple consiste généralement en une paire {entrée, sortie attendue}, illustrant la tâche que le LLM doit accomplir. Cette technique est particulièrement utile lorsque la tâche est nouvelle pour le modèle ou lorsque l'on souhaite guider précisément le format ou le style de la sortie.

**Sélection d'exemples (Exemplar Selection) :**
La performance du few-shot learning dépend de manière critique de la qualité et de la pertinence des exemples choisis.

*   **Représentativité et Pertinence :** Les exemples doivent être hautement représentatifs de la tâche cible et du type d'entrée que le modèle recevra en inférence. Ils doivent clairement illustrer le mappage souhaité de l'entrée vers la sortie.[24]
*   **Qualité du formatage :** Les exemples doivent non seulement montrer la logique de la tâche mais aussi le format de sortie désiré (par exemple, JSON, liste à puces, style de paragraphe spécifique).[24, 25] La cohérence dans le formatage entre les exemples et la requête cible est essentielle.
*   **Diversité vs Similarité :** Il peut y avoir un compromis entre fournir des exemples très similaires à la requête cible (pour un guidage précis) et fournir des exemples plus diversifiés (pour montrer l'étendue de la tâche). La stratégie optimale peut dépendre de la complexité de la tâche.
*   **Rôle de l'exactitude des étiquettes :** Des recherches [14, 21] suggèrent que l'exactitude absolue des étiquettes (la sortie) dans les exemples few-shot n'est pas toujours le facteur le plus déterminant. La distribution des entrées, le format des exemples, et la clarté du "pattern" qu'ils illustrent peuvent avoir un impact plus significatif sur la capacité du modèle à généraliser à partir de ces exemples.
*   **Sélection basée sur des motifs de raisonnement :** Pour les tâches impliquant un raisonnement complexe (comme dans le CoT), la sélection d'exemples peut être améliorée en se concentrant sur la diversité et la clarté des "motifs de raisonnement" qu'ils présentent, plutôt que sur la simple similarité sémantique de la question.[14, 16]

**"Ordering Effects" (Effets d'ordre) :**
Un aspect notable et parfois contre-intuitif du few-shot learning est la sensibilité du LLM à l'ordre dans lequel les exemples sont présentés dans le prompt.[21] Changer la permutation des quelques exemples fournis peut conduire à des variations significatives dans la performance du modèle sur la même tâche et la même requête cible.

*   **Cause :** Ces effets d'ordre sont une manifestation de la nature séquentielle du traitement de l'information par les Transformers et de la manière dont le mécanisme d'attention pondère le contexte. Les exemples vus plus récemment (plus proches de la requête cible dans le prompt) pourraient avoir une influence disproportionnée (biais de récence), ou certains ordres pourraient créer des interférences ou des synergies inattendues dans la manière dont le modèle construit sa compréhension de la tâche.
*   **Implications :** Cela signifie qu'il ne suffit pas de choisir de bons exemples ; leur arrangement optimal peut également nécessiter une exploration.

**Optimisation et Bonnes Pratiques :**

*   **Itération sur les exemples et leur ordre :** Expérimenter avec différents ensembles d'exemples et différentes permutations pour trouver la combinaison qui produit les meilleurs résultats.
*   **Clarté et concision :** S'assurer que chaque exemple est aussi clair, concis et non ambigu que possible. Éliminer toute information superflue.
*   **Cohérence :** Maintenir une structure et un formatage cohérents à travers tous les exemples et pour la question cible.[24, 26]
*   **Commencer par le Zero-Shot :** Une bonne pratique générale est de commencer par une approche zero-shot (aucune démonstration). Si cela ne fonctionne pas de manière satisfaisante, passer au few-shot. Si le few-shot est insuffisant, alors envisager des techniques plus coûteuses comme le fine-tuning.[25]
*   **Délimiteurs :** Utiliser des délimiteurs clairs (comme `###` ou `"""`) pour séparer les instructions, le contexte, les exemples, et la requête cible peut aider le modèle à mieux parser le prompt.[25]

Le few-shot learning, en tant qu'instance d'ICL, est soumis aux mêmes limitations fondamentales discutées précédemment (section 2.2), notamment les "boundary values" et les contraintes imposées par la Layer Normalization et Softmax. Par conséquent, les exemples choisis ne doivent pas seulement être pertinents pour la tâche, mais aussi se situer dans une plage de complexité et de distribution de données que le modèle peut effectivement traiter et à partir de laquelle il peut généraliser.

### 2.4 Prompt chaining et décomposition multi-étapes

Face à des tâches complexes, tenter de tout résoudre avec un unique prompt monolithique s'avère souvent inefficace et conduit à des résultats de qualité médiocre. Le "prompt chaining" (chaînage de prompts) ou la décomposition multi-étapes offre une alternative plus robuste et gérable.[27, 28] Cette approche consiste à diviser la tâche globale en une séquence de sous-tâches plus simples et plus ciblées. La sortie générée par le LLM pour une étape devient alors l'entrée, ou une partie du contexte, pour le prompt de l'étape suivante.

**Méthodologies de décomposition et de chaînage :**

*   **Décomposition séquentielle simple :** C'est la forme la plus basique, où les sous-problèmes sont résolus l'un après l'autre. Par exemple, pour écrire un rapport :
    1.  Prompt 1 : "Génère un plan détaillé pour un rapport sur [sujet]."
    2.  Prompt 2 (utilisant le plan du Prompt 1) : "Rédige la section introduction du rapport basé sur ce plan : [plan]."
    3.  Prompt 3 (utilisant l'introduction) : "Rédige la section développement..." et ainsi de suite.
    Cette approche est mentionnée dans [27] comme "Overloading the Prompt with Multiple Tasks - The fix? Break complex requests into smaller chunks."
*   **Décomposition hiérarchique / récursive :** Pour des problèmes très complexes, la décomposition peut elle-même être hiérarchique. Un problème principal est décomposé en sous-problèmes majeurs, qui peuvent à leur tour être décomposés en sous-sous-problèmes plus granulaires.[17, 29] La technique de "Decomposition" testée dans [17] pour le raisonnement scientifique demandait au LLM de décomposer récursivement les problèmes complexes en sous-problèmes simples et gérables, qui étaient ensuite envoyés au modèle pour résolution pas à pas.
*   **Context-Aware Decomposition (CAD) :** Proposée dans un fil de discussion [29], cette technique avancée met l'accent non seulement sur la décomposition en composants, mais aussi sur la nécessité pour le modèle de maintenir une "conscience" des interrelations entre ces composants lors de la synthèse des solutions partielles. Le prompt guide le LLM à :
    1.  Identifier les composants clés du problème.
    2.  Pour chaque composant : expliquer son importance, identifier l'approche pour le résoudre, et le résoudre.
    3.  Synthétiser les solutions partielles en adressant explicitement leurs interactions.
    4.  Fournir une solution holistique.
    Un "journal de pensée" peut être demandé pour suivre le raisonnement à chaque étape.[29]
*   **Recursive Self-Improvement (RSIP) :** Bien que principalement une technique d'auto-critique (voir section 2.5 et 7.6), RSIP [29] implique une séquence d'opérations (Générer -> Critiquer -> Améliorer -> Répéter) qui peut être vue comme une forme de chaînage où la sortie d'une étape (l'amélioration) devient l'entrée de la suivante (la nouvelle critique).
*   **Chain-of-Thought (CoT) comme forme de décomposition :** Le CoT lui-même (section 2.1) est une forme de décomposition, où le "problème" est de générer une réponse correcte, et les "sous-tâches" sont les étapes de raisonnement intermédiaires.[15]

**Cas d'usage typiques :**
Le chaînage de prompts est particulièrement adapté pour :
*   Les tâches de raisonnement complexes qui nécessitent plusieurs inférences logiques ou calculs.
*   La génération de contenu long et structuré (rapports, articles, scénarios) où chaque section peut être traitée comme une sous-tâche.
*   La simulation de dialogues multi-tours où le contexte doit être maintenu et enrichi à chaque tour.
*   Les applications qui nécessitent une forme de planification ou la résolution de problèmes en plusieurs phases.
*   L'analyse de données où une première étape peut être l'extraction d'informations, suivie d'une synthèse, puis d'une interprétation.

**Avantages :**

*   **Meilleure gérabilité :** Des tâches plus petites sont plus faciles à définir et à prompter.
*   **Fiabilité et précision accrues :** Le LLM peut se concentrer sur un objectif plus restreint à chaque étape, réduisant le risque d'erreurs ou de sorties hors sujet.
*   **Débogage facilité :** Si le résultat final est incorrect, il est plus aisé d'isoler l'étape (le prompt spécifique de la chaîne) qui a échoué.[28]
*   **Contrôle granulaire :** Permet d'injecter une logique humaine ou des vérifications entre les étapes.
*   **Réduction de la charge cognitive pour le LLM :** Évite de surcharger le modèle avec trop d'instructions ou un contexte trop vaste en une seule fois.

L'utilisation de la décomposition et du chaînage de prompts s'inscrit dans une tendance plus large visant à orchestrer les capacités des LLM de manière plus structurée, se rapprochant d'une forme de "programmation" où les prompts individuels agissent comme des fonctions ou des modules au sein d'un flux de travail plus large.

### 2.5 Constitutional AI et impact RLHF sur le prompting

L'alignement des Grands Modèles de Langage avec les valeurs et préférences humaines est un enjeu majeur. Deux approches prédominantes pour cet alignement, le Reinforcement Learning from Human Feedback (RLHF) et la Constitutional AI (CAI), ont des implications significatives sur la manière dont les modèles répondent aux prompts et, par conséquent, sur les stratégies de prompting à adopter.

**Reinforcement Learning from Human Feedback (RLHF) :**
Le RLHF est une technique largement utilisée pour affiner les LLM afin qu'ils produisent des réponses plus utiles, honnêtes et inoffensives (souvent résumé par le principe HHH : Helpful, Honest, Harmless).[30] Le processus typique implique :
1.  La collecte de données de préférences humaines : des évaluateurs humains comparent et notent différentes réponses du LLM à un même prompt.
2.  L'entraînement d'un modèle de récompense : ce modèle apprend à prédire quelle réponse les humains préféreraient.
3.  Le fine-tuning du LLM : le LLM est ensuite affiné en utilisant l'apprentissage par renforcement, où le modèle de récompense fournit le signal de feedback pour guider le LLM vers la génération de réponses mieux notées.
Le RLHF a un impact notable sur le comportement des modèles, les rendant plus "naturels" dans leurs interactions conversationnelles et plus enclins à suivre les instructions de manière coopérative.[30] Cependant, le RLHF est coûteux en termes de collecte de données humaines, peut manquer de transparence quant aux raisons exactes des préférences agrégées, et sa robustesse face à des prompts adverses ou non anticipés reste un défi.[30, 31]

**Constitutional AI (CAI) :**
Introduite par Anthropic [31], la Constitutional AI est une approche d'alignement alternative qui vise à réduire la dépendance à une supervision humaine directe et massive à chaque étape. Dans la CAI, le modèle apprend à critiquer et à réviser ses propres sorties en se basant sur un ensemble de principes explicites ou de règles, formant une "constitution".[31]

Le processus CAI se déroule typiquement en deux phases principales [31] :
1.  **Phase d'apprentissage supervisé (critique et révision) :** Le modèle est d'abord entraîné à générer des critiques de ses propres réponses (ou des réponses d'un autre modèle) par rapport aux principes de la constitution. Ensuite, il est entraîné à réviser les réponses pour les rendre conformes à ces critiques et donc à la constitution.
2.  **Phase d'apprentissage par renforcement (préférence) :** Un modèle de préférence est entraîné sur la base des réponses révisées (celles qui sont plus alignées avec la constitution étant préférées). Ce modèle de préférence est ensuite utilisé pour affiner davantage le LLM via RL, de manière similaire au RLHF, mais où le signal de "préférence" est dérivé de l'alignement avec la constitution plutôt que directement des labels humains.

Une application pratique de la CAI, notamment pour des modèles plus petits ou dans des contextes où un cycle complet d'entraînement n'est pas possible, peut impliquer un processus en trois étapes à l'inférence [31, 32] :
1.  Le modèle génère une réponse initiale à un prompt (qui pourrait être potentiellement problématique).
2.  Le modèle est ensuite prompté pour critiquer sa propre réponse à la lumière de la constitution (ou d'un sous-ensemble de principes pertinents).
3.  Enfin, le modèle est prompté pour réécrire sa réponse initiale en tenant compte de la critique formulée, afin de la rendre conforme.

**Impact sur les stratégies de prompting :**
L'alignement, que ce soit par RLHF ou CAI, modifie la sensibilité du modèle aux prompts, en particulier concernant les aspects de sécurité, d'éthique, de ton et de respect des instructions.

*   Pour les modèles alignés par **RLHF**, les prompts qui sont clairs, non ambigus et qui évitent les formulations pouvant être interprétées comme cherchant à contourner les garde-fous de sécurité ont tendance à mieux fonctionner. Les modèles sont souvent entraînés à être plus prudents ou à refuser de répondre à des requêtes perçues comme nuisibles.
*   Pour la **Constitutional AI**, le "prompting constitutionnel" devient une technique en soi. La manière dont la constitution est formulée et dont les étapes de critique et de révision sont promptées est cruciale pour l'efficacité du processus, surtout avec des modèles plus petits qui nécessitent un guidage plus explicite et structuré.[31, 32]
    *   **Stratégies de prompt spécifiques pour la CAI sur petits LLM (issues de [31, 32]) :**
        *   Dans le prompt de critique, inclure la nuance "si nécessaire" dans les conditions de refus (par exemple, "recommander de refuser de répondre *si nécessaire*") pour éviter que le modèle ne critique à tort des réponses inoffensives.
        *   Énumérer explicitement les catégories de contenu nuisible (par exemple, "non éthique, dangereux, illégal") pour guider l'évaluation du modèle.
        *   Dans le prompt de critique ou de révision, rappeler au modèle le prompt utilisateur initial pour maintenir le contexte et s'assurer que la réponse révisée reste pertinente.
        *   Le prompt de révision doit encourager un refus poli et explicatif si la question originale est jugée problématique, plutôt qu'un simple refus sec.

**Efficacité et limitations :**
La CAI a montré sa capacité à réduire significativement la génération de réponses nuisibles.[31] Cependant, son succès dépend de la capacité intrinsèque du modèle à effectuer une auto-évaluation précise et à détecter le contenu problématique. Cette capacité peut varier considérablement en fonction de l'architecture du modèle et de son entraînement initial. Par exemple, des études [31, 32] ont montré que les modèles basés sur Llama étaient plus performants dans l'application de la CAI que des modèles comme Qwen2.5, qui avaient tendance à échouer plus souvent dans la phase de critique. Le RLHF, bien que puissant, reste une méthode coûteuse et dont les mécanismes de décision internes peuvent manquer de transparence.[30, 31]

Le tableau suivant compare l'impact de RLHF et CAI sur les stratégies de prompting :

| Méthode d'Alignement | Principe de Fonctionnement | Impact sur le Comportement du Modèle | Stratégies de Prompting Spécifiques | Avantages pour l'Alignement | Défis et Limitations |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **RLHF (Reinforcement Learning from Human Feedback)** | Entraînement d'un modèle de récompense basé sur les préférences humaines, puis fine-tuning du LLM par RL avec ce modèle de récompense.[30] | Réponses plus "naturelles", coopératives, alignées sur les critères HHH (Helpful, Honest, Harmless). Tendance à la prudence/refus pour requêtes perçues comme nuisibles. | Prompts clairs, non ambigus. Éviter les formulations cherchant à contourner les garde-fous. Spécifier le ton et le niveau de détail attendus. | Alignement direct sur les jugements humains. Amélioration de la conversationnalité. | Coûteux en collecte de données humaines, manque de transparence du modèle de récompense, peut être "cassant" face à des prompts non vus.[30, 31] |
| **Constitutional AI (CAI)** | Le modèle apprend à critiquer et réviser ses sorties sur la base d'une "constitution" (ensemble de principes), réduisant la dépendance aux labels humains directs.[31] | Réponses alignées sur les principes de la constitution. Capacité d'auto-correction. | **Prompting Constitutionnel :** Formulation précise de la constitution. Prompts de critique structurés (énumérer catégories nuisibles, rappeler le prompt initial). Prompts de révision encourageant le refus poli et expliqué.[31, 32] | Moins dépendant des labels humains à grande échelle, plus transparent si la constitution est claire, potentiellement plus scalable. | Efficacité dépend de la capacité d'auto-évaluation du modèle, qui varie selon l'architecture.[31, 32] La conception de la constitution et des prompts de critique/révision est cruciale. |

L'évolution des techniques de prompting avancées révèle une tendance vers la simulation de processus métacognitifs au sein des LLM. Des approches comme le CoT, l'auto-critique (inhérente à la CAI et à des techniques comme RSIP), et d'autres méthodes émergentes (Self-Refine, Self-Verification) ne se contentent pas de donner des ordres au LLM, mais structurent une forme de "dialogue interne" ou de "processus de réflexion" simulé. Cela suggère que l'ingénierie de prompts de demain pourrait moins ressembler à la rédaction d'instructions qu'à la conception de protocoles de collaboration cognitive avec le modèle.

## 3. OPTIMISATION ALGORITHMIQUE DES PROMPTS

Alors que l'ingénierie manuelle des prompts reste une compétence précieuse, elle peut s'avérer laborieuse, subjective et difficile à généraliser. L'optimisation algorithmique des prompts vise à automatiser, en tout ou en partie, le processus de découverte et de raffinement des prompts afin d'atteindre des performances optimales de manière plus systématique, reproductible et potentiellement au-delà de ce que l'intuition humaine peut concevoir.

### 3.1 Optimisation basée sur le gradient : AutoPrompt, OPRO, GPO, TextGrad, nouvelles méthodes

Ces méthodes s'inspirent des techniques d'optimisation par descente de gradient, couramment utilisées pour l'entraînement des modèles de deep learning, afin de rechercher les prompts les plus efficaces. Elles peuvent opérer soit dans l'espace discret des tokens qui composent un prompt, soit dans l'espace continu des embeddings de ces tokens.

*   **AutoPrompt :** L'une des approches pionnières, AutoPrompt se concentre sur la découverte automatique d'un ensemble de "trigger tokens" discrets (jetons déclencheurs). Ces tokens, lorsqu'ils sont ajoutés à l'entrée originale, sont optimisés pour maximiser la probabilité de la sortie désirée, en particulier pour des tâches de classification. L'optimisation se fait généralement en recherchant dans l'espace des tokens pour trouver ceux qui, combinés, produisent le gradient le plus favorable vers la bonne réponse.
*   **OPRO (Optimization by PROmpting) :** Cette technique innovante utilise le LLM lui-même comme agent optimiseur.[33] Le processus est itératif : le LLM (l'optimiseur) génère un ensemble de prompts candidats pour une tâche donnée. Ces prompts sont ensuite évalués (par exemple, en mesurant leur performance sur un jeu de données de validation, souvent en faisant appel à un autre LLM, le modèle cible). Les scores de performance obtenus sont ensuite fournis en retour au LLM optimiseur, qui les utilise comme information pour générer une nouvelle série de prompts, espérons-le améliorés. Ce cycle se poursuit jusqu'à ce qu'un critère d'arrêt soit atteint.
*   **GPO (Gradient-inspired LLM-based Prompt Optimizer) :** GPO établit une analogie formelle entre les optimiseurs basés sur le gradient (utilisés pour l'apprentissage des poids des modèles) et la conception d'optimiseurs de prompts qui sont eux-mêmes basés sur des LLM.[34, 35, 36, 37] GPO identifie deux composantes cruciales dans le processus d'optimisation, transposées du domaine de l'optimisation des paramètres :
    1.  **Direction de mise à jour (Update Direction) :** Déterminée en récupérant des prompts pertinents à partir de la "trajectoire d'optimisation" (c'est-à-dire l'historique des prompts testés et de leurs performances). Les prompts les plus performants ou ceux montrant une amélioration servent de guide.
    2.  **Méthode de mise à jour (Update Method) :** Implique un raffinage basé sur la génération (le LLM optimiseur réécrit ou améliore le prompt actuel en s'inspirant de la direction de mise à jour) et un contrôle de l'ampleur des modifications (par exemple, en limitant la "distance d'édition" entre les prompts successifs, potentiellement via une stratégie de décroissance de type cosinus pour cette contrainte).[34, 36]
    Le cœur de GPO réside dans la conception d'un "méta-prompt" qui instruit le LLM optimiseur sur la manière d'appliquer ces principes pour générer de meilleurs prompts pour le modèle cible.[34] Ce méta-prompt inclut typiquement la trajectoire des prompts passés (triés par performance) comme démonstration et des instructions pour le raffinage et le contrôle de la distance d'édition.[34]
*   **TextGrad :** Ce framework [33] permet d'optimiser des prompts textuels en calculant des "gradients textuels". L'idée est d'utiliser un LLM pour estimer comment des modifications apportées au texte du prompt (ajout, suppression, remplacement de mots) influenceraient une métrique de performance donnée. Ces "gradients" indiquent alors la direction dans laquelle le prompt devrait être modifié pour s'améliorer.
*   **metaTextGrad :** Allant plus loin, metaTextGrad [33] propose un "méta-optimiseur" conçu pour améliorer les performances des optimiseurs de prompts existants (comme TextGrad, OPRO, ou MIPRO). Il comprend deux composantes : un "meta prompt optimizer" qui affine les prompts utilisés par les optimiseurs LLM eux-mêmes (c'est-à-dire les méta-prompts), et un "meta structure optimizer" qui explore des manières de combiner ou de séquencer différents optimiseurs de prompts pour une tâche donnée.[33]

**Nouvelles méthodes (2023-2024) :** La recherche dans ce domaine est très active. Les efforts se concentrent sur l'amélioration de l'efficacité de l'optimisation (réduire le nombre d'appels LLM nécessaires, qui peuvent être coûteux), la capacité à optimiser des prompts plus longs et plus structurés (au-delà de simples phrases), et l'application à une plus grande variété de tâches et de modèles. Des techniques de "soft prompt tuning" (optimisation de prompts continus dans l'espace des embeddings) continuent également d'être explorées, bien qu'elles nécessitent souvent un accès plus direct au modèle que les API publiques ne le permettent.[38]

### 3.2 Approches évolutionnaires pour l'optimisation de prompts

Les approches évolutionnaires s'inspirent des principes de l'évolution naturelle et des algorithmes génétiques pour "faire évoluer" une population de prompts vers des solutions de plus en plus performantes pour une tâche donnée. Ces méthodes traitent les prompts comme des "individus" qui peuvent se reproduire (croisement) et subir des modifications aléatoires (mutation).

*   **EvoPrompt (Microsoft Research) :** Cette approche, décrite dans [39, 40], utilise un Grand Modèle de Langage (LLM) pour implémenter les opérateurs clés d'un algorithme évolutionnaire, typiquement un algorithme génétique (AG). Le processus général est le suivant :
    1.  **Initialisation :** Une population de prompts initiaux est créée (soit manuellement, soit générée aléatoirement ou par un LLM).
    2.  **Évaluation (Fitness Calculation) :** Chaque prompt de la population est évalué sur la tâche cible. Sa "fitness" (aptitude) est mesurée par un score de performance (par exemple, exactitude, score F1, ou une évaluation humaine/LLM).
    3.  **Sélection :** Des prompts "parents" sont sélectionnés à partir de la population actuelle, généralement en privilégiant ceux qui ont une meilleure fitness (par exemple, via une méthode de sélection par la roulette, où la probabilité de sélection est proportionnelle à la fitness).[39, 40]
    4.  **Croisement (Crossover) :** Deux prompts parents sont choisis. Le LLM reçoit ces deux prompts ainsi que des instructions spécifiques sur la manière de les "croiser" pour produire un ou plusieurs prompts "enfants". Ces instructions peuvent demander au LLM de combiner des éléments des deux parents, de prendre le début de l'un et la fin de l'autre, ou d'autres formes de recombinaison sémantique.[39, 40]
    5.  **Mutation :** Un prompt enfant (ou un prompt existant) subit une mutation. Le LLM reçoit le prompt et des instructions pour y introduire de petites modifications aléatoires mais sémantiquement cohérentes (par exemple, reformuler une phrase, remplacer des synonymes, ajouter ou supprimer une clause).[39, 40]
    6.  **Remplacement/Itération :** Les nouveaux prompts enfants remplacent les individus moins performants de la population, ou la population est étendue puis réduite. Le processus (évaluation, sélection, croisement, mutation) est répété sur plusieurs générations, dans l'espoir que la fitness moyenne de la population s'améliore avec le temps.[39, 40]

*   **Avantages des approches évolutionnaires :**
    *   **Scalabilité et facilité d'implémentation relative :** Une fois les opérateurs de croisement et de mutation basés sur LLM définis, le processus peut être largement automatisé.[39, 40]
    *   **Performance :** Peuvent surpasser l'ingénierie manuelle des prompts, découvrant des formulations non intuitives mais efficaces.[39, 40]
    *   **Boîte noire :** Ne nécessitent pas l'accès aux gradients, aux poids internes ou aux logits du modèle cible, ce qui les rend applicables aux LLM accessibles uniquement via des API.
    *   **Exploration robuste :** Les mécanismes de mutation et de croisement permettent une exploration diversifiée de l'espace des prompts.

*   **Défis :**
    *   **Coût de l'évaluation :** L'étape d'évaluation de la fitness de chaque prompt peut être très coûteuse si elle implique de nombreux appels au LLM cible, surtout pour de grandes populations et sur de nombreuses générations.
    *   **Conception des opérateurs :** Définir des instructions de croisement et de mutation qui soient à la fois créatives et pertinentes pour générer des prompts de haute qualité en langage naturel est un défi d'ingénierie de prompt en soi.
    *   **Convergence :** Comme pour tous les algorithmes évolutionnaires, il n'y a pas de garantie de convergence vers l'optimum global, et le processus peut parfois stagner dans des optima locaux.

*   **Autres approches et mentions :**
    *   Le document "The Prompt Report" [18] et "A Survey of Automatic Prompt Engineering" [38] listent GrIPS (Gradientfree Instructional Prompt Search) comme une méthode utilisant des algorithmes évolutionnaires pour l'optimisation de prompts.
    *   Des commentaires [40] suggèrent des extensions où l'algorithme évolutionnaire optimise le "system prompt" qui, à son tour, guide la génération de prompts spécifiques à la tâche, introduisant un niveau de méta-optimisation.

Les approches évolutionnaires représentent une direction prometteuse pour l'automatisation de l'ingénierie des prompts, en particulier lorsque l'accès interne au modèle est limité. Leur succès dépendra de l'efficacité des opérateurs génétiques basés sur LLM et de la gestion du coût computationnel de l'évaluation.

Le tableau suivant compare les approches d'optimisation de prompts basées sur le gradient et évolutionnaires :

| Approche | Principe de Base | Type d'Optimisation | Avantages | Inconvénients/Défis | Exemple de Cas d'Usage |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **GPO (Gradient-inspired LLM-based Prompt Optimizer)** [34, 36] | LLM optimiseur guidé par un méta-prompt inspiré des concepts de direction et méthode de mise à jour des optimiseurs gradient. | Discret (espace des tokens). | Guidage structuré par analogie avec l'optimisation des modèles, peut être efficace. | Nécessite un LLM optimiseur capable, la conception du méta-prompt est clé. | Optimisation de prompts pour tâches de classification ou génération où une trajectoire d'amélioration peut être définie. |
| **OPRO (Optimization by PROmpting)** [33] | Le LLM lui-même génère et évalue itérativement des prompts candidats. | Discret (espace des tokens). | Conceptuellement simple, utilise les capacités du LLM pour l'optimisation. | Peut être coûteux en appels LLM, la stratégie de génération/évaluation du LLM optimiseur est cruciale. | Découverte de prompts pour de nouvelles tâches où peu d'a priori existent. |
| **TextGrad** [33] | Calcule des "gradients textuels" via un LLM pour guider la modification du prompt. | Discret (espace des tokens), avec une notion de "gradient". | Fournit une direction de modification plus explicite que la simple génération. | La notion de "gradient textuel" est une approximation, peut être complexe à estimer. | Raffinement de prompts existants où des petites modifications ciblées sont souhaitées. |
| **EvoPrompt (Algorithmes Évolutionnaires)** [39, 40] | Utilise un LLM pour implémenter les opérateurs d'un algorithme génétique (sélection, croisement, mutation) sur une population de prompts. | Discret (espace des tokens). | Scalable, ne nécessite pas d'accès aux gradients du modèle cible, explore un large espace de prompts. | Coût d'évaluation de la fitness élevé, conception des opérateurs LLM pour croisement/mutation. | Optimisation de prompts pour des tâches complexes où l'espace de recherche est vaste et l'accès au modèle cible est limité (API). |

La convergence de ces techniques d'optimisation avec la notion de "méta-optimisation" (où l'optimiseur lui-même est optimisé) suggère une complexification et une sophistication croissantes dans la quête du prompt parfait. L'ingénieur ML pourrait à l'avenir se concentrer davantage sur la conception de ces méta-optimiseurs ou sur la définition des fonctions de fitness et des contraintes pour les processus d'optimisation automatisés, plutôt que sur la rédaction manuelle de chaque prompt.

### 3.3 Benchmarking : métriques, datasets Big-Bench, HELM

Le benchmarking joue un rôle indispensable dans l'évaluation objective et la comparaison des performances des Grands Modèles de Langage (LLM), des différentes techniques de prompting, et des stratégies d'optimisation de prompts. Il fournit un cadre standardisé pour mesurer les progrès et identifier les forces et faiblesses des approches existantes.

**Métriques clés pour l'évaluation :**
La nature des tâches effectuées par les LLM étant variée, un ensemble diversifié de métriques est nécessaire :

*   **Précision (Accuracy) :** Fondamentale pour les tâches de classification, les questions à choix multiples (QCM), et toute tâche où une réponse correcte unique existe.[41, 42]
*   **Scores basés sur le chevauchement de N-grammes :**
    *   **BLEU (Bilingual Evaluation Understudy) :** Principalement utilisé pour l'évaluation de la traduction automatique, mesure la similarité avec des traductions de référence.
    *   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation) :** Utilisé pour l'évaluation du résumé automatique, compare les n-grammes du résumé généré avec ceux des résumés de référence.
    *   **METEOR (Metric for Evaluation of Translation with Explicit ORdering) :** Une autre métrique pour la traduction, prenant en compte la synonymie et l'ordre des mots.
    Bien que largement utilisés, ces scores peuvent parfois mal refléter la qualité sémantique ou la cohérence globale, surtout pour des textes longs ou créatifs.[41]
*   **Similarité sémantique :** Des métriques comme BERTScore ou SentenceBERT utilisent des embeddings pour comparer la similarité sémantique entre la sortie du modèle et une référence, capturant mieux les paraphrases et les variations de formulation que les métriques basées sur les n-grammes.[41]
*   **Métriques spécifiques à la tâche :**
    *   **IFEval Strict Accuracy :** Utilisée dans le benchmark HELM pour évaluer la capacité d'un modèle à suivre précisément des instructions complexes.[42]
    *   **WB-Score (WildBench Score) :** Utilisée dans HELM pour évaluer la qualité des dialogues, souvent basée sur une évaluation par un autre LLM puissant ("LLM-as-a-judge").[42]
*   **Évaluation humaine :** Reste la référence ultime, en particulier pour les tâches subjectives comme la génération créative, l'évaluation de la cohérence, de la pertinence, du ton, ou de l'innocuité. Elle est cependant coûteuse et difficile à mettre à l'échelle.
*   **LLM-as-a-judge :** Une tendance croissante consiste à utiliser un LLM très performant (par exemple, GPT-4o, Claude 3.5 Sonnet) pour évaluer les sorties d'autres modèles, en lui fournissant des critères d'évaluation.[42] Cela offre une alternative plus scalable à l'évaluation humaine, bien qu'elle introduise ses propres biais potentiels.
*   **Autres métriques :** Selon le contexte, des métriques de robustesse (performance face à des perturbations de l'entrée), d'équité (absence de biais), d'efficacité (temps de réponse, coût computationnel) sont également cruciales.

**Datasets et Frameworks de Benchmark Majeurs :**

*   **BIG-Bench (Beyond the Imitation Game Benchmark) :**
    *   **Objectif :** Évaluer un large éventail de capacités et de limitations des LLM à travers une collection de plus de 204 tâches diverses, couvrant des domaines comme les mathématiques, la linguistique, la physique, le raisonnement de bon sens, et la détection de biais sociaux.[43]
    *   **Contributeurs :** Développé par une large collaboration (444 auteurs de 132 institutions).[43]
    *   **Modèles évalués :** Conçu pour évaluer divers types de modèles, y compris les modèles GPT d'OpenAI, les architectures Transformer denses internes à Google, et les Transformers épars de type Switch, sur une gamme de tailles allant de quelques millions à des centaines de milliards de paramètres.[43]
    *   **Métrique :** Bien que le "Task-Tuned Score (TTS)" soit mentionné [43], les détails de son calcul ne sont pas fournis dans les extraits disponibles.[44] Les évaluations se basent souvent sur l'exactitude pour des tâches spécifiques.
*   **BIG-Bench Hard (BBH) :**
    *   Un sous-ensemble de BIG-Bench comprenant 23 tâches jugées particulièrement difficiles pour les LLM au moment de sa création.[44, 45]
    *   **Saturation :** Avec les progrès rapides des LLM, BBH est devenu "saturé", les modèles de pointe atteignant des scores très élevés (plus de 90% pour certains), réduisant son utilité pour discriminer les capacités des modèles les plus récents.[44, 45]
*   **BIG-Bench Extra Hard (BBEH) :**
    *   **Introduction :** Proposé en 2024 pour pallier la saturation de BBH et continuer à pousser les limites de l'évaluation du raisonnement des LLM.[44, 45, 46, 47]
    *   **Méthodologie :** BBEH remplace chacune des 23 tâches de BBH par une nouvelle tâche homologue qui sonde des capacités de raisonnement similaires mais avec une difficulté significativement accrue.[44] Ces nouvelles tâches exigent des compétences plus avancées telles que le raisonnement multi-sauts (many-hop reasoning), l'apprentissage à la volée (learning on the fly), la détection d'erreurs dans des traces de raisonnement, le traitement de longs contextes (avec des "aiguilles dans une botte de foin"), la capacité à aller à l'encontre de forts a priori, la gestion des dépendances à longue portée, et la gestion des distracteurs.[44, 46, 47]
    *   **Construction :** Les tâches ont été développées itérativement, avec des ajustements basés sur les performances de modèles de référence jusqu'à ce que leur exactitude tombe en dessous d'un certain seuil (par exemple, 70%).[47]
    *   **Performance actuelle :** Les meilleurs modèles généralistes atteignent une exactitude moyenne harmonique d'environ 9.8% sur BBEH, et les meilleurs modèles spécialisés en raisonnement environ 44.8%, indiquant une marge de progression substantielle.[44, 45]
*   **HELM (Holistic Evaluation of Language Models) - Stanford CRFM :**
    *   **Philosophie :** Un framework pour une évaluation large, multi-métrique, standardisée et transparente des modèles de langage, avec des résultats continuellement mis à jour.[41, 42]
    *   **Principes clés :** Large couverture de scénarios et de capacités, utilisation de multiples métriques pour chaque scénario, standardisation des méthodes d'évaluation pour la reproductibilité, et transparence totale au niveau du prompt (les prompts exacts utilisés sont publiés).[41, 42]
    *   **HELM Capabilities :** Une itération récente (mars 2025 dans [42]) qui se concentre sur un ensemble organisé de scénarios mesurant des capacités fondamentales. Les scénarios sont sélectionnés en fonction de leur non-saturation par les modèles SOTA, de leur récence, et de leur qualité (clarté, adoption par la communauté).[42] La v1.0.0 évalue des capacités comme :
        *   Connaissance Générale (MMLU-Pro) : Métrique = Exactitude.[42]
        *   Raisonnement (GPQA) : Métrique = Exactitude.[42]
        *   Suivi d'Instructions (IFEval) : Métrique = IFEval Strict Accuracy.[42]
        *   Dialogue (WildBench) : Métrique = WB-Score (LLM-as-a-judge).[42]
        *   Raisonnement Mathématique (Omni-MATH) : Métrique = Exactitude jugée par LLM.[42]
    *   **MedHELM :** Une adaptation du framework HELM spécifiquement pour les applications médicales, regroupant les tâches en catégories pertinentes pour le domaine de la santé (aide à la décision clinique, génération de notes cliniques, etc.).[41]

Le tableau suivant résume les principaux benchmarks :

| Benchmark | Organisation/Origine | Objectif Principal | Types de Tâches Incluses | Métriques Clés Utilisées | Points Forts | Limitations/Considérations |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **BIG-Bench** [43] | Collaboration large (Google, universités, etc.) | Évaluation large des capacités et limitations des LLM. | 204 tâches : maths, linguistique, biais social, raisonnement, etc. | Task-Tuned Score (TTS), exactitude. | Très grande diversité de tâches, couvre de nombreux aspects. | Certaines tâches peuvent être simples pour les LLM récents. Le TTS n'est pas clairement défini dans les sources. |
| **BIG-Bench Hard (BBH)** [44] | Sous-ensemble de BIG-Bench | Évaluation sur des tâches jugées difficiles. | 23 tâches difficiles de BIG-Bench. | Exactitude. | Standard de facto pour le raisonnement général pendant un temps. | Devenu saturé par les modèles SOTA.[44, 45] Nombre élevé de tâches à choix binaire/limité.[44] |
| **BIG-Bench Extra Hard (BBEH)** [44, 46, 47] | Mehran Kazemi et al. (Google AI et collaborateurs) | Adresser la saturation de BBH avec des tâches de raisonnement plus difficiles. | 23 nouvelles tâches remplaçant celles de BBH, exigeant raisonnement multi-sauts, apprentissage à la volée, etc. | Exactitude (souvent moyenne harmonique). | Repousse les limites de l'évaluation du raisonnement, forte difficulté. | Encore récent, les scores sont bas, indiquant un défi majeur pour les LLM actuels. |
| **HELM (Holistic Evaluation of Language Models)** [41, 42] | Stanford CRFM | Évaluation holistique, multi-métrique, standardisée et transparente. | Large éventail de scénarios couvrant de multiples capacités (raisonnement, connaissance, dialogue, sécurité, etc.). | Exactitude, F1, ROUGE, BERTScore, IFEval, WB-Score, etc. (multi-métrique). | Transparence (prompts publiés), reproductibilité, mises à jour continues, couverture large. | La complexité de l'évaluation holistique peut rendre la comparaison directe sur un seul score difficile. |
| **HELM Capabilities** [42] | Stanford CRFM | Évaluation ciblée de capacités clés avec des scénarios récents et non saturés. | Connaissance (MMLU-Pro), Raisonnement (GPQA), Suivi d'Instructions (IFEval), Dialogue (WildBench), Maths (Omni-MATH). | Exactitude, IFEval Strict Acc., WB-Score, Exactitude jugée par LLM. | Utilise des scénarios de pointe, se concentre sur des capacités fondamentales, transparent. | Sélection de scénarios peut évoluer. Dépendance à LLM-as-a-judge pour certaines métriques. |

L'existence et l'évolution de ces benchmarks sont un moteur de progrès. Cependant, une focalisation excessive sur des benchmarks spécifiques peut entraîner un risque de "sur-optimisation" des modèles et des prompts pour ces tâches précises, potentiellement au détriment de leur capacité à généraliser à des scénarios du monde réel non couverts. Il est donc crucial pour les ingénieurs ML de compléter les résultats des benchmarks par des évaluations spécifiques à leurs propres cas d'usage, notamment via des A/B tests rigoureux.

### 3.4 Méthodologies A/B testing statistiquement rigoureuses

L'A/B testing est une méthode expérimentale cruciale pour comparer de manière empirique l'efficacité de différentes versions de prompts (ou d'autres composantes d'un système LLM, comme le modèle lui-même ou ses paramètres) dans des conditions d'utilisation réelles.[48] Une mise en œuvre statistiquement rigoureuse est essentielle pour obtenir des conclusions fiables, surtout compte tenu de la nature stochastique des sorties des LLM.

**Éléments clés d'un A/B Test pour Prompts LLM [48] :**

1.  **Allocation aléatoire des utilisateurs (Randomized User Allocation) :** Les utilisateurs sont assignés de manière aléatoire à différents groupes (variantes). Typiquement, un groupe "contrôle" (A) reçoit le prompt existant ou la version de référence, et un groupe "traitement" (B) reçoit le nouveau prompt ou la version modifiée. L'allocation est souvent 50/50, mais d'autres répartitions sont possibles. La randomisation est fondamentale pour assurer que les groupes sont comparables et que les différences observées ne sont pas dues à des biais de sélection.
2.  **Isolation d'une seule variable (Single Variable Isolation) :** Pour attribuer de manière non ambiguë les changements de performance à la modification du prompt, il est impératif que toutes les autres conditions de l'expérience restent constantes entre les groupes. Cela signifie utiliser le même modèle LLM, les mêmes paramètres (température, top-p, etc.), et le même contexte applicatif. Si plusieurs changements sont testés simultanément (par exemple, un nouveau prompt ET une nouvelle température), il devient impossible de déterminer la cause d'une éventuelle différence de performance.
3.  **Déploiements incrémentiels (Incremental Rollouts / Canary Releases) :** Pour minimiser les risques associés à un changement potentiellement moins performant ou problématique, il est recommandé de commencer l'A/B test sur un petit sous-ensemble du trafic (par exemple, 1%, 5%). Si les résultats initiaux sont prometteurs et qu'aucun effet négatif majeur n'est observé, le pourcentage de trafic exposé peut être progressivement augmenté. Des plateformes d'expérimentation modernes ou des systèmes de feature flags facilitent cette gestion.

**Métriques nuancées à suivre pour les LLM [48, 49] :**
L'évaluation de la performance des prompts LLM va au-delà des simples taux de clics ou de conversion :

*   **Latence et Débit :**
    *   *Importance :* Des réponses lentes peuvent entraîner l'abandon par l'utilisateur.
    *   *Mesures :* Temps jusqu'au premier token (TTFT), temps total de complétion de la réponse, taux d'erreur ou de timeout.
*   **Engagement Utilisateur :**
    *   *Exemples :* Longueur des conversations (pour les chatbots), durée moyenne de session, fréquence d'utilisation répétée, taux de rétention.
    *   *Signification :* Un engagement plus élevé suggère généralement une expérience utilisateur plus précieuse ou agréable.
*   **Qualité de la Réponse du Modèle :**
    *   **Évaluations humaines explicites :** Collecter des retours directs des utilisateurs (par exemple, notes de 1 à 5, questions "Cette réponse a-t-elle été utile?").
    *   **Proxies comportementaux :** Suivre des indicateurs indirects comme la fréquence à laquelle les utilisateurs copient la sortie du LLM, la modifient, demandent une régénération de la réponse, ou abandonnent une tâche après l'interaction.
    *   **Scores d'évaluation de qualité :** Si des évaluateurs (humains ou LLM) sont intégrés au processus, leurs scores peuvent être suivis (par exemple, via des outils comme Langfuse [49]).
*   **Coût et Efficacité :**
    *   *Exemples :* Nombre de tokens utilisés par requête (entrée et sortie), coût monétaire par millier de requêtes, utilisation des ressources GPU.
    *   *Compromis :* Il est crucial d'équilibrer la performance avec le coût. Un prompt qui améliore légèrement la qualité mais double le coût pourrait ne pas être viable.
Il est souvent utile de définir une métrique principale (Overall Evaluation Criterion - OEC) et plusieurs métriques secondaires ou "garde-fous" (guardrail metrics) pour s'assurer que l'amélioration d'un aspect ne se fait pas au détriment critique d'un autre.

**Conception de l'expérience et rigueur statistique [48] :**

1.  **Hypothèse et objectif mesurable :** Définir clairement ce que l'on s'attend à améliorer et dans quelle mesure. Par exemple : "Le prompt B, qui inclut un exemple de format de sortie, augmentera le taux de réponses correctement formatées de 10% par rapport au prompt A, sans augmenter significativement la latence."
2.  **Estimation de la taille de l'échantillon (Sample Size Estimation) :** Avant de lancer le test, effectuer une analyse de puissance (power analysis) pour déterminer le nombre d'observations (utilisateurs, sessions, requêtes) nécessaires dans chaque groupe pour détecter un effet de la taille souhaitée avec une signification statistique et une puissance données (par exemple, p-value < 0.05, puissance de 80%). Ceci est particulièrement important avec les LLM en raison de la variabilité inhérente de leurs réponses.
3.  **Randomisation et durée :** S'assurer que l'assignation aux groupes est véritablement aléatoire et que les utilisateurs restent dans le même groupe pendant toute la durée de leur interaction ou de l'expérience. L'expérience doit durer suffisamment longtemps pour capturer des comportements utilisateurs représentatifs et lisser les variations journalières ou hebdomadaires.
4.  **Enregistrement et collecte de données (Logging) :** Mettre en place un système robuste pour enregistrer toutes les données pertinentes : quel utilisateur a vu quelle variante, les prompts soumis, les réponses générées, les métriques de performance, les retours utilisateurs, etc.
5.  **Analyse statistique :**
    *   Pour les métriques continues (par exemple, note moyenne, latence), des tests t (si les hypothèses sont respectées) ou des équivalents non-paramétriques (par exemple, test de Mann-Whitney U) peuvent être utilisés.
    *   Pour les métriques catégorielles ou binaires (par exemple, taux de succès/échec, réponse correctement formatée oui/non), des tests du chi-carré ou des tests Z de comparaison de deux proportions sont appropriés.
    *   Calculer les p-valeurs et les intervalles de confiance pour évaluer la signification statistique des différences observées. Il est crucial d'éviter de "jeter un coup d'œil" aux résultats et d'arrêter le test prématurément dès qu'une différence semble apparaître (ce qui augmente le risque de faux positifs).
6.  **Évaluation de la signification et des compromis :** Une différence statistiquement significative n'est pas toujours pratiquement significative. Évaluer l'ampleur de l'effet et considérer les compromis (par exemple, une amélioration de 3% de la satisfaction justifie-t-elle une augmentation de 50% du coût?).
7.  **Décision et Itération :** Si la variante traitement (B) surpasse clairement le contrôle (A) sur la métrique principale tout en respectant les garde-fous, elle peut être déployée plus largement. Si les résultats sont non concluants ou négatifs, il faut analyser les raisons, raffiner l'hypothèse ou le prompt, et potentiellement lancer une nouvelle itération de test.

**Outils :** Des plateformes comme Langfuse [49] offrent des fonctionnalités pour étiqueter différentes versions de prompts (par exemple, `prod-a`, `prod-b`) et suivre les métriques de performance associées (latence, coût, scores d'évaluation) pour faciliter l'analyse des A/B tests. Statsig [48] est une plateforme d'expérimentation en ligne qui fournit un cadre pour de tels tests.

**Quand utiliser l'A/B testing pour les prompts :**
Cette méthodologie est particulièrement pertinente lorsque [49] :
*   L'application dispose déjà de métriques de succès claires et mesurables.
*   L'application traite une grande variété d'entrées utilisateur, rendant l'évaluation sur des jeux de données statiques moins représentative.
*   Une certaine variabilité de performance peut être tolérée pendant la phase de test.
*   Après des tests approfondis sur des données de test internes, on souhaite valider les changements sur un segment d'utilisateurs réels avant un déploiement complet (canary deployment).

Le tableau suivant présente un cadre pour un A/B testing rigoureux des prompts LLM :

| Étape du Processus A/B Test | Description Détaillée | Points d'Attention Spécifiques aux LLM | Outils Utiles |
| :--- | :--- | :--- | :--- |
| **1. Définition Hypothèse/Objectif** | Formuler une hypothèse claire, spécifique et mesurable sur l'impact attendu du changement de prompt. | L'hypothèse doit considérer la nature stochastique des LLM et viser des améliorations robustes. | Outils de documentation, brainstorming. |
| **2. Conception Expérience** | Allocation aléatoire des utilisateurs, isolation de la variable (prompt), stratégie de déploiement incrémentiel. | S'assurer que la randomisation est correcte. Maintenir tous les autres paramètres LLM (modèle, température) identiques. | Plateformes d'expérimentation (Statsig), feature flags. |
| **3. Sélection Métriques** | Choisir des métriques primaires et secondaires (garde-fous) couvrant latence, engagement, qualité de réponse, coût. | Les métriques de qualité peuvent être subjectives ; envisager des proxies comportementaux ou des évaluations humaines/LLM. | Systèmes de logging, outils d'analyse (Langfuse pour le suivi des prompts). |
| **4. Estimation Taille Échantillon** | Calculer le nombre d'observations nécessaires par groupe pour une puissance statistique adéquate. | La variabilité des réponses LLM peut nécessiter des échantillons plus grands que pour des A/B tests web traditionnels. | Calculateurs de taille d'échantillon en ligne, bibliothèques statistiques (Python, R). |
| **5. Exécution & Monitoring** | Lancer le test, surveiller les métriques en temps réel pour détecter des problèmes majeurs. | S'assurer que le logging des données est complet et précis pour chaque variante. | Tableaux de bord de monitoring, systèmes d'alerte. |
| **6. Analyse Statistique** | Appliquer les tests statistiques appropriés (t-tests, chi-carré, etc.). Calculer p-valeurs et intervalles de confiance. | Tenir compte des comparaisons multiples si plusieurs variantes sont testées. Ne pas arrêter prématurément. | Logiciels statistiques (R, Python avec `scipy.stats`, `statsmodels`), plateformes d'expérimentation avec capacités d'analyse. |
| **7. Décision & Itération** | Interpréter les résultats, prendre une décision (déployer, rejeter, itérer). Documenter les apprentissages. | Évaluer la signification pratique en plus de la signification statistique. Planifier la prochaine itération si nécessaire. | Systèmes de gestion des connaissances, outils de suivi d'expériences. |

L'intégration de l'A/B testing dans le cycle de vie du développement des prompts est une marque de maturité dans l'ingénierie des LLM, permettant une amélioration continue et basée sur des données probantes.

## 4. SPÉCIFICITÉS DU PROMPTING PAR MODÈLE

L'efficacité d'un prompt n'est pas universelle ; elle est intimement liée aux caractéristiques spécifiques du Grand Modèle de Langage (LLM) auquel il est soumis. Les ingénieurs doivent adapter leurs stratégies en fonction de l'architecture du modèle, de sa capacité à gérer des contextes de différentes tailles, de ses aptitudes multimodales et de son intégration avec des outils externes.

### 4.1 Stratégies différenciées selon l'architecture

Comme souligné précédemment (section 1.3), les architectures des LLM (GPT, Claude, Gemini, LLaMA, etc.) présentent des différences notables en termes de taille de fenêtre de contexte, de nature des données d'entraînement, de philosophies d'alignement (RLHF, Constitutional AI), et de capacités spécialisées (accès web, multimodalité). Ces distinctions imposent des approches de prompting différenciées.

*   **Modèles avec de grandes fenêtres de contexte (par exemple, Claude 3.5 Sonnet avec 200K tokens) :**
    *   Ces modèles permettent d'inclure une quantité substantielle d'informations directement dans le prompt, comme des documents entiers, des transcriptions de conversations longues, ou des ensembles d'instructions très détaillés.[6]
    *   **Stratégie :** Fournir un maximum de contexte pertinent. Cependant, il faut être conscient du phénomène de "perte au milieu" (lost in the middle), où
